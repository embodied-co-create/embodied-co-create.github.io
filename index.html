<!DOCTYPE html>
<html lang="en">

<head>
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap" rel="stylesheet">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>RSS Embodied Co-Creation 2025</title>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <!-- Custom styles for this template -->
    <link href="./css/scrolling-nav.css" rel="stylesheet">
    <link href="./css/style.css" rel="author stylesheet">
    <style>
        .pc-column {
		    width:270px;
		    display:inline-block;
		    vertical-align: top;
		}
		
		.pc_list_item {
		    display:inline-block;
		    width:200px;
		}
	
	</style>
</head>

<body id="page-top">
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light" id="mainNav">
        <div class="container bar-container">
            <a class="title-head" href="#page-top">RSS Embodied Co-Creation 2025</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#about">About</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#speakers">Speakers</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#schedule">Schedule</a>
                    </li>
                    <!-- <li class="nav-item">
                    		    <a class="nav-link js-scroll-trigger" href="#callpapers">Call for Papers </a>
				</li> -->
                    <li class="nav-item">
			<a class="nav-link js-scroll-trigger" href="#papers">Accepted Papers</a>
		    </li>
                    <!-- <li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#faq">FAQ</a>
				</li> -->
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#organizers">Organizers</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="/2024.html">2024</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <header class="headercontainer bg-primary text-white" style="padding: 0%; max-height: none; ">
        <div style="background-color: rgba(160,160,160,0.0)" class="text-center">
            <div style="padding-bottom: 6%; padding-top: 6%; background-image: url('./images/banner_full.png'); background-size: cover; background-position: center">
                <div class="container titlebox" ; style="display: inline-block; background-color:rgba(0,0,0, 0.7); width:auto;">
                    <p style="text-align: center; margin-bottom: 2" class="title">Workshop on <b>Embodied Co-Creation: Robotic Tools That Make and Perform Arts</b></p>
                    <p style="text-align: center; margin-bottom: 0" class="subtitle"><a href="https://roboticsconference.org" target="_blank">Robotics Science and Systems Conference (RSS 2025)</a> <br> Olin Hall of Engineering (OHE) #122, USC Campus, June 21, Los Angeles, USA</p>
                    <p style="text-align: center; margin-bottom: 0" class="subtitle">Half-day Workshop</p>
                </div>
            </div>
        </div>
    </header>
    <hr class="half-rule" />
    <section id="about" style="padding:70px 0 0 0">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <!-- div style="justify-content: center; text-align:center;"><img src="https://semrob.github.io/images/semrob_2024_logo.png" width="700px"></div-->
                    <span class=titlesec>About</span>
                    <br>
                    <span>
                        The objective of this workshop is to foster dialogue between robotics researchers and creative practitioners to explore how humanoids and other embodied systems can support new forms of artistic expression and human-machine co-creation. We aim to identify concrete technical needs and design opportunities for integrating robots into complex and real-world creative workflows, with an emphasis on direct collaboration between human and robot.
                        </br>
                        </br>
                        Tasks such as painting, choreography, sculpture, and fabrication demand tightly coupled solutions across perception, whole-body control, and intuitive human-robot interaction — making them uniquely rich environments for advancing the capabilities of dexterous and adaptive robotic systems while supporting creative needs.
                        </br>
                        </br>
                        The workshop will address the following topics:
                        <ul>
                            <li>Design mechanisms for creative robotics that facilitate expressive form factors for supporting artistic movements, e.g., compliance of soft materials</li>
                            <li>Whole-body planning and control, tailored to the temporal and semantic structure of creative tasks</li>
                            <li>Robot learning for creative tasks, including policy learning, skill acquisition, and adaptation to aesthetics feedback, in both artifact production or performance-based domains</li>
                            <li>Teleoperation and human-robot interfaces, with an emphasis on intuitive and ergonomic control suited for non-expert users</li>
                            <li>The role of generative models in embodied co-creation, including models that predict motion, gestures or physical forms such as sketches or sculptures</li>
                            <li>Applications such as robot painting, choreography, fabrication</li>
                        </ul>
                        </br>
                        Through invited talks, showcases, and panel discussion, the workshop aims to surface shared research directions and promote meaningful collaboration between researchers and creators.
                    </span>
                </div>
            </div>
        </div>
    </section>
    <section id="event">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class=titlesec>Event Information</span>
                    <span>
                        This is a half-day workshop, held at the <i>2025 Robotics Science and Systems conference</i> (RSS), in Los Angeles, United States on <b>21 June 2025</b>.
			</br>
			</br>
			The workshop location is room #122, in the <a href="https://www.google.com/maps/place/Olin+Hall,+3650+McClintock+Ave,+Los+Angeles,+CA+90089/@34.0206887,-118.292074,838m/data=!3m2!1e3!4b1!4m6!3m5!1s0x80c2c7fca3be47df:0x34ec8b40e042d9e3!8m2!3d34.0206843!4d-118.2894991!16s%2Fg%2F11c5nmb8fx?entry=tts&g_ep=EgoyMDI1MDYwNC4wIPu8ASoASAFQAw%3D%3D&skid=d6040080-ebdf-4e0f-b0f7-9ba30f9cbefd" target="_blank"><b>Olin Hall of Engineering (OHE)</b></a>, at the University of Southern California (USC).
			</br>
			</br>
			Here is a map that includes the workshop location: <a href="https://roboticsconference.org/images/local2025/workshops.png" target="_blank">https://roboticsconference.org/images/local2025/workshops.png</a>
			</br>
			</br>
			This workshop is part of the official RSS 2025 workshop program.		
                    </span>
                </div>
            </div>
        </div>
    </section>
    <hr class="half-rule" />
    <section id="speakers">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class=titlesec> Speakers and Panelists</span><br>
                    <div class="row">
                        <a href="https://jessethomason.com/" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/jesse.png" class="figure-img img-fluid ">
                                <p class=profname> Jesse Thomason </p>
                                <p class=institution>University of Southern California</p>
                            </div>
                        </a>
                        <a href="https://dorsa.fyi" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/dorsa.png" class="figure-img img-fluid ">
                                <p class=profname> Dorsa Sadigh </p>
                                <p class=institution> Stanford University </p>
                            </div>
                        </a>
                        <a href="https://talkingtorobots.com/yonatanbisk.html" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src='./images/speakers/bisk.jpg' class="figure-img img-fluid ">
                                <p class=profname> Yonatan Bisk </p>
                                <p class=institution> Carnegie Mellon University </p>
                            </div>
                        </a>
                        </a>
                        <a href="https://tedxiao.me/" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src='./images/speakers/ted.png' class="figure-img img-fluid ">
                                <p class=profname> Ted Xiao </p>
                                <p class=institution>Google DeepMind</p>
                            </div>
                        </a>
                    </div>
                    <div class="row">
                        <a href="https://msavva.github.io" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/manolis.png" class="figure-img img-fluid ">
                                <p class=profname> Manolis Savva </p>
                                <p class=institution> Simon Fraser University </p>
                            </div>
                        </a>
                        <a href="https://www.lerrelpinto.com" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/lerrel.png" class="figure-img img-fluid ">
                                <p class=profname> Lerrel Pinto </p>
                                <p class=institution> New York University </p>
                            </div>
                        </a>
                        <a href="https://ai.uni-bremen.de/team/benjamin_alt" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/benjamin.jpeg" class="figure-img img-fluid ">
                                <p class=profname> Benjamin Alt </p>
                                <p class=institution> Bremen University </p>
                            </div>
                        </a>
		    </div>
                </div>
            </div>
    </section>
	<hr class="half-rule" />
    <section class="">
        <div class="container" id="schedule">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class="titlesec"><span></span>Schedule</span>
                    <br><br>
                    <table class="table table-striped">
                        <tbody>
                            <tr>
                                <th style="width: 21%"> Time </th>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 08:30
                                    <td />
                                <td> Organizers <br> <b> Introductory Remarks </b> </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 08:40
                                    <td />
                                <td> Keynote 1: Jesse Thomason <br> <b> Embracing Language as Grounded Communication </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstract-JT" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstract-JT" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        Language is not text data, it is a human medium for communication. The larger part of the natural language processing (NLP) community has doubled down on treating digital text as a sufficient approximation of language, scaling datasets and corresponding models to fit that text. I have argued that experience in the world grounds language, tying it to objects, actions, and concepts. In fact, I believe that language carries meaning only when considered alongside that world, and that the zeitgeist in NLP research currently misses the mark on truly interesting questions at the intersection of human language and machine computation. In this talk, I’ll highlight some of the ways my lab enables agents and robots to better understand and respond to human communication by considering the grounded context in which that communication occurs, including neurosymbolic multimodal reasoning, natural language dialogue and interaction for lifelong learning, and utilizing NLP technologies on non-text communication.
					    <br><br>
					    Keynote references:<br>
					    <a target="_blank" href="https://arxiv.org/abs/2406.02791">PSALM</a><br>
					    <a target="_blank" href="https://progprompt.github.io">ProgPrompt</a><br>
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 09:00
                                    <td />
                                <td> Keynote 2: Manolis Savva <br> <b> Towards Realistic & Interactive 3D Simulation for Embodied AI </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstract-MS" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstract-MS" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        3D simulators are increasingly being used to develop and evaluate "embodied AI" (agents perceiving and acting in realistic environments). Much of the prior work in this space has treated simulators as "black boxes" within which learning algorithms are to be deployed. However, the system characteristics of the simulation platforms themselves and the datasets that are used with these platforms both greatly impact the feasibility and the outcomes of experiments involving simulation. In this talk, I will describe several recent projects that outline emerging challenges and opportunities in the development of 3D simulation for embodied AI.
					    <br><br>
                                        Bio: Manolis Savva is an Associate Professor at Simon Fraser University, and a Canada Research Chair in Computer Graphics. His research focuses on analysis, organization and generation of 3D content. Prior to his current position he was a visiting researcher at Facebook AI Research, and a postdoctoral researcher at Princeton University. He received his Ph.D. from Stanford University under the supervision of Pat Hanrahan. His work has been recognized through several awards including an ACM UIST notable paper award (ReVision), an ICCV best paper nomination (Habitat), two SGP dataset awards (ShapeNet, SGP 2018; ScanNet, SGP 2020), the 2022 Graphics Interface early career researcher award, and an ICLR 2023 outstanding paper award (Emergence of Maps).
					    <br><br>
					    Keynote references:<br>
					    <a target="_blank" href="https://3dlg-hcvc.github.io/hssd/">Habitat Synthetic Scenes Dataset (HSSD)</a><br>
					    <a target="_blank" href="https://3dlg-hcvc.github.io/smc/">SceneMotifCoder</a><br>
					    <a target="_blank" href="https://3dlg-hcvc.github.io/s2o/">S2O: Static to Openable</a><br>
					    <a target="_blank" href="https://3dlg-hcvc.github.io/cage/">CAGE: Controllable Articulation GEneration</a><br>
					    <a target="_blank" href="https://3dlg-hcvc.github.io/singapo/">SINGAPO</a><br>
                                    </div>
                                </td>
                            </tr>				
                            <tr>
                                <td style="width: 21%"> 09:20
                                    <td />
                                <td> Keynote 3: Dorsa Sadigh <br> <b> Human-Aligned Robot Learning: manipulation policies via preferences, RLHF, and VLM feedback </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstract-DS" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstract-DS" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        Abstract TBD
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 09:40
                                    <td />
                                <td> Spotlight Talks: #1, #4, #13, #17 </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 10:00
                                    <td />
                                <td> Keynote 4: Yonatan Bisk <br> <b> Semantics? Reasoning? Can we define either of those terms? </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstract-YB" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstract-YB" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        In this talk I'll discuss some recent work on language conditioned robotics, but I might also choose to spend time questioning the basic assumptions of all of our work, and if we're all misguided about the important questions in robotics.
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 10:20
                                    <td />
                                <td> Keynote 5: Ted Xiao <br> <b> Full-stack Robotics Foundation Models: From Embodied Reasoning to Dexterity </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstractmatthew" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstractmatthew" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        Advances in data-driven robot learning have accelerated progress towards general purpose robotic control. While improvements in Vision Language Action (VLA) models and large-scale imitation learning have enabled early multipurpose robotic foundation models, progress is often a direct reflection of the robot training dataset distribution or bespoke algorithmic adjustments. This stands in stark contrast to trends in multimodal frontier models, where capability improvements come not only from nuanced small-scale design decisions, but from properly harnessing the fundamental intelligence scaling laws of the underlying frontier model. In this talk, I will discuss how perspectives from frontier modeling can inspire and guide robotics research. In particular, I will cover how Gemini Robotics tackles robotics from a truly full-stack approach: how improving multimodal frontier model capabilities like embodied reasoning results in a generalizable, steerable, and dexterous robot foundation model.
					    <br><br>
					    Keynote references:<br>
					    <a target="_blank" href="https://deepmind.google/models/gemini-robotics/">https://deepmind.google/models/gemini-robotics/</a><br>
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 10:40
                                    <td />
                                <td> Keynote 6: Benjamin Alt <br> <b> Semantic Digital Twins for Robust and Flexible Robot Behavior </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#benjamin" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="benjamin" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
					    Keynote references:<br>
					    <a target="_blank" href="https://arxiv.org/abs/2504.13159">Digital Twin Generation from Visual Data: A Survey</a><br>
					    <a target="_blank" href="https://github.com/ndrwmlnk/awesome-digital-twins">Awesome Digital Twins</a><br>

                                    </div>
                                </td>
                            </tr>				
                            <tr>
                                <td style="width: 21%"> 10:50
                                    <td />
                                <td> Coffee Break, Socializing, Posters</td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 11:30
                                    <td />
                                <td> Keynote 7: Lerrel Pinto <br> <b> On Building General-Purpose Home Robots </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstract-LP" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstract-LP" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        The concept of a "generalist machine" in homes — a domestic assistant that can adapt and learn from our needs, all while remaining cost-effective — has long been a goal in robotics that has been steadily pursued for decades. In this talk, I will present our recent efforts towards building such capable home robots. First, I will discuss how large, pretrained vision-language models can induce strong priors for mobile manipulation tasks like pick-and-drop. But pretrained models can only take us so far. To scale beyond basic picking, we will need systems and algorithms to rapidly learn new skills. This requires creating new tools to collect data, improving representations of the visual world, and enabling trial-and-error learning during deployment. While much of the work presented focuses on two-fingered hands, I will briefly introduce learning approaches for multi-fingered hands which support more dexterous behaviors and rich touch sensing combined with vision. Finally, I will outline unsolved problems that were not obvious initially, which, when solved, will bring us closer to general-purpose home robots.
					    <br><br>
					    Keynote references:<br>
					    <a target="_blank" href="https://robotutilitymodels.com/">Robot Utility Models</a><br>
					    <a target="_blank" href="https://egozero-robot.github.io/">EgoZero: Robot Learning from Smart Glasses</a><br>
					    <a target="_blank" href="https://dynamem.github.io/">DynaMem</a><br>
					    <a target="_blank" href="https://point-policy.github.io/">Point Policy</a><br>
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 11:50
                                    <td />
                                <td> Debate: Implicit/Data-emergent Reasoning Capabilities versus Explicit Reasoning Mechanisms? <br> <b> Panelists: Jesse Thomason, Ted Xiao, Manolis Savva, Lerrel Pinto, Yonatan Bisk, Benjamin Alt</b> </td>
                            </tr>
                            <tr>
                                <td style="width: 21%">
                                    12:30
                                    <td />
                                <td> Organizers <br> <b> Closing Remarks </b>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </section>
    <br>
    <hr class="half-rule" />
    <section id="callpapers" style="padding: 70px 0 50px 0;">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class=titlesec>Call for Papers</span><br>
                    <h5 style="font-weight: bold"> Targeted Topics </h5>
                    In addition to the <a target="_blank" href="https://roboticsconference.org/information/cfp/">RSS subject areas</a>, we especially invite paper submissions on various topics, including (but not limited to):
                    <br><br>
                    <ul>
                        <li>Mechanical design for creative robotics</li>
                        <li>Whole-body control of humanoid robots in creative tasks</li>
                        <li>Robot learning for creative tasks</li>
                        <li>Teleoperation and/or human-robot interfaces for creative tasks</li>
                        <li>The role of generative models in embodied co-creation</li>
                        <li>Applications such as robot painting, choreography, fabrication, etc.</li>
                        <li>Design mechanisms for creative robotics that facilitate expressive form factors</li>
                        <li>Whole-body planning and control tailored to creative tasks</li>
                        <li>Robot learning for creative tasks including policy learning and skill acquisition</li>
                        <li>Adaptation to aesthetics feedback in artifact production or performance-based domains</li>
                    </ul>
                    <h5 style="font-weight: bold"> Submission Guidelines </h5>
                    This workshop suggests <b>4+N or 8+N paper length</b> formats — i.e., 4 or 8 pages of main content with unlimited additional pages for references, appendices, etc. However, like RSS 2025, we impose no strict page length requirements on submissions; we trust that authors will recognize that respecting reviewers' time is helpful to the evaluation of their work.<br><br>
                    Submissions will be handled through CMT: <a href="#" target="_blank">Link to be announced</a><br>
                    <p style="color: #AAAAAA; size:10px"><i>(Required acknowledgement: the Microsoft CMT service was used for managing the peer-reviewing process for this conference. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.)</i></p><br>
                    We will accept the official <a target="_blank" href="https://roboticsconference.org/docs/paper-template-latex.tar.gz">LaTeX</a> or <a target="_blank" href="https://roboticsconference.org/docs/paper-template-word.zip">Word</a> paper templates, provided by RSS 2025.
                    <br><br>
                    Our review process will be <b>double-blind</b>, following the RSS paper submission policy for Science/Systems papers.
                    <br><br>
                    All accepted papers will be invited for poster presentations; the highest-rated papers, according to the Technical Program Committee, will be given spotlight presentations. Accepted papers will be made available online on this workshop website as <b>non-archival</b> reports, allowing authors to also submit their works to future conferences or journals. We will highlight the Best Paper Award during the closing remarks at the workshop event.<br><br>
                    <h5 style="font-weight: bold"> Important Dates </h5>
                    <ul>
                        <li style="display: list-item">
                            <b>Submission deadline:</b><font color="#000000"><b>TBD</b></font>, 23:59 AOE.
                        </li>
                        <li style="display: list-item">
                            <b>Author Notifications:</b> TBD.
                        </li>
                        <li style="display: list-item">
                            <b>Camera Ready:</b> TBD.
                        </li>
                        <li style="display: list-item">
                            <b>Workshop:</b> 21 June 2025, Half-day
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        </div>
        </div>
    </section>
    <hr class="half-rule" />
    <section id="papers">
		<div class="container">
			<div class="row">
			    <div class="col-md-10 mx-auto">
					<span class=titlesec>Accepted Papers</span><br>
				    	<p>Congratulations to Paper #13 (<i>WoMAP: World Models For Embodied Open-Vocabulary Object Localization</i>) for winning the Best Paper Award and for Paper #1 (<i>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models</i>) for winning the Best Paper Runner-up!</p>

					<ul class="listpapers">
						<li>

							(Paper ID #1) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper01.pdf" target="_blank">Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models</a>
							<span style="color: #ff8c00">(spotlight)</span><span style="color: #ff0000"> (best paper runner-up) </span>
							<br>
							<span class="authorname">
							Xiaoyang Shi; Brian Ichter; Michael Equi; Liyiming Ke; Karl Pertsch; Quan Vuong; James Tanner; Anna Walling; Haohuan Wang; Niccolo Fusai; Adrian Li-Bell; Danny Driess; Lachy Groom; Sergey Levine; Chelsea Finn
							</span>
						</li>
						<li>
							(Paper ID #2) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper2.pdf" target="_blank">VERDI: VLM-Embedded Reasoning for Autonomous Driving</a>
							<br>
							<span class="authorname">
							Zhiting Mei; Bowen Feng; Baiang Li; Julian Ost; Roger Girgis; Anirudha Majumdar; Felix Heide
							</span>
						</li>
						<li>
							(Paper ID #3) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper03.pdf" target="_blank">STRIVE: Structured Representation Integrating VLM Reasoning for Efficient Object Navigation</a>
							<span class="authorname">
								Haokun Zhu; Zongtai Li; Zhixuan Liu; Wenshan Wang; Ji Zhang; Jonathan Francis; Jean Oh
							</span>
						</li>
						<li>
							(Paper ID #4) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper04.pdf" target="_blank">Flexible Multitask Learning with Factorized Diffusion Policy</a>
							<span style="color: #ff8c00">(spotlight)</span><br>
							<span class="authorname">
								Chaoqi Liu; Haonan Chen; Sigmund H. Høeg; Shaoxiong Yao; Yunzhu Li; Kris Hauser; Yilun Du
							</span>
						</li>
						<li>
							(Paper ID #5) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper5.pdf" target="_blank">RayFronts: Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration</a>
							<br>
							<span class="authorname">
							Omar Alama; Avigyan Bhattacharya; Haoyang He; Seungchan Kim; Yuheng Qiu; Wenshan Wang; Cherie Ho; Nikhil Keetha; Sebastian Scherer
							</span>
						</li>
						<li>
							(Paper ID #6) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper06.pdf" target="_blank">EgoZero: Robot Learning from Smart Glasses</a>
						 	<br>
							<span class="authorname">
							Vincent Liu*; Ademi Adeniji*; Haotian Zhan*; Siddhant Haldar; Raunaq Bhirangi; Pieter Abbeel; Lerrel Pinto
							</span>
						</li>
						<li>
							(Paper ID #7) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper07.pdf" target="_blank">Touch begins where vision ends: Generalizable policies for contact-rich manipulation</a>
							<br>
							<span class="authorname">
							Zifan Zhao; Siddhant Haldar; Jinda Cui; Lerrel Pinto; Raunaq Bhirangi
							</span>
						</li>


						<li>
							(Paper ID #8) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper08.pdf" target="_blank">Feel the Force: Contact-Driven Learning from Humans</a>
							<br>
							<span class="authorname">
							Ademi Adeniji; Zhuoran Chen; Vincent Liu; Venkatesh Pattabiraman; Siddhant Haldar; Raunaq Bhirangi; Pieter Abbeel; Lerrel Pinto
							</span>
						</li>
						<li>
							(Paper ID #9) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper09.pdf" target="_blank">IMPACT: Intelligent Motion Planning with Acceptable Contact Trajectories via Vision-Language Models</a>
							<br>
							<span class="authorname">
							Yiyang Ling; Karan Owalekar; Oluwatobiloba Adesanya; Erdem Bıyık; Daniel Seita
							</span>
						</li>
						<li>

							(Paper ID #10) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper10.pdf" target="_blank">Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation</a>
							<br>
							<span class="authorname">
							Siddhant Haldar; Lerrel Pinto
						</span>
						</li>

						<li>
							(Paper ID #11) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper11.pdf" target="_blank">GRIM: Task-Oriented Grasping with Conditioning on Generative Examples</a>
							<br>
							<span class="authorname">
							Shailesh Shailesh; Alok Raj; Nayan Kumar; Priya Shukla; Andrew Melnik; Michael Beetz; Gora Chand Nandi
						</span>
						</li>

						<li>
							(Paper ID #12) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper12.pdf" target="_blank">Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning</a>
							<br>
							<span class="authorname">
							Sigmund Høeg; Chaoqi Liu; Yilun Du; Olav Egeland
						</span>
						</li>

						<li>
							(Paper ID #13) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper13.pdf" target="_blank">WoMAP: World Models For Embodied Open-Vocabulary Object Localization</a>
							<span style="color: #ff8c00">(spotlight)</span><span style="color: #ff0000"> (best paper award)</span>
							<br>
							<span class="authorname">
							Tenny Yin; May Mei; Tao Sun; Lihan Zha; Jeremy Bao; Emily Zhou; Miyu Yamane; Ola Shorinw; Anirudha Majumdar
						</span>
						</li>

						<li>
							(Paper ID #14) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper14.pdf" target="_blank">Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agents</a>
							<br>
							<span class="authorname">
							Che Rin Yu; Daewon Chae; Dabin Seo; Yoonha Jang; Sangwon Lee; Hyeongwoo IM; Jinkyu Kim
						</span>
						</li>

						<li>
							(Paper ID #15) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper15.pdf" target="_blank">CASPER: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models</a>
							<br>
							<span class="authorname">
							Huihan Liu; Rutav Shah; Shuijing Liu; Jack Pittenger; Mingyo Seo; Yuchen Cui; Yonatan Bisk; Roberto Martin-Martin; Yuke Zhu
						</span>
						</li>

						<li>
							(Paper ID #16) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper16.pdf" target="_blank">Grounding Language Models with Semantic Digital Twins for Robotic Planning</a>
							<br>
							<span class="authorname">
							Mehreen Naeem; Andrew Melnik; Michael Beetz
						</span>
						</li>

						<li>
							(Paper ID #17) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper17.pdf" target="_blank">MotIF: Motion Instruction Fine-tuning</a>
							<span style="color: #ff8c00">(spotlight)</span>
							<br>
							<span class="authorname">
							Minyoung Hwang; Joey Hejna; Dorsa Sadigh; Yonatan Bisk
						</span>
						</li>

						<li>
							(Paper ID #18) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper18.pdf" target="_blank">Mixed Initiative Dialog for Human-Robot Collaborative Mobile Manipulation</a>
							<br>
							<span class="authorname">
							Albert Yu; Chengshu Li; Luca Macesanu; Arnav Balaji; Ruchira Ray; Ray Mooney; Roberto Roberto Martín-Martín
						</span>
						</li>
						<li>
							(Paper ID #19) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper19.pdf" target="_blank">GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic Guidance</a>
							<br>
							<span class="authorname">
								Arthur Fender Coelho Bucker; Pablo Ortega Kral; Jonathan Francis; Jean Oh
							</span>
						</li>
						<li>
							(Paper ID #20) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper20.pdf" target="_blank">Points2Reward: Robotic Manipulation Rewards from Just One Video</a>
							<br>
							<span class="authorname">
								Junyao Shi; Joshua Smith; Jianing Qian; Dinesh Jayaraman
							</span>
						</li>
						<li>
							(Paper ID #21) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper21.pdf" target="_blank">Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining</a>
							<br>
							<span class="authorname">
								Yaru Niu; Yunzhe Zhang; Mingyang Yu; Changyi Lin; Chenhao Li; Yikai Wang; Yuxiang Yang; Wenhao Yu; Tingnan Zhang; Zhenzhen Li; Jonathan Francis; Bingqing Chen; Jie Tan; Ding Zhao
							</span>
						</li>
						<li>
							(Paper ID #22) <a class="linkpaper" href="./docs/2025_rss_semrob.github.io_paper22.pdf" target="_blank">GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering</a>
							<br>
							<span class="authorname">
								Blake Buchanan; Saumya Saxena; Chris Paxton; Bingqing Chen; Jonathan Francis; Narunas Vaskevicius; Luigi Palmieri; Peiqi Liu; Oliver Kroemer
							</span>
						</li>
					</ul>

				</div>
			</div>
		</div>
	</section> 
    <hr class="half-rule" />
    <section id="organizers">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class=titlesec>Organizers</span><br>
                    <div class="row">
                        <a href="mailto:yejink@allenai.org" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/yejin.jpg class="figure-img img-fluid ">
                                <p class=profname> Yejin Kim* </p>
                                <p class=institution> Allen Institute for AI (Ai2) </p>
                                <p class=institution> yejink@allenai.org </p>
                            </div>
                        </a>
                        <a href="mailto:yingkewang@stanford.edu" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/yingke.jpg class="figure-img img-fluid ">
                                <p class=profname> Yingke Wang </p>
                                <p class=institution> Stanford University </p>
                                <p class=institution> yingkewang@stanford.edu </p>
                            </div>
                        </a>
                        <a href="mailto:kyj1107@hyundai.com" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/kwon.jpg class="figure-img img-fluid ">
                                <p class=profname> Kwon Yong Jin </p>
                                <p class=institution> ZER01NE, Hyundai Motor Company </p>
                                <p class=institution> kyj1107@hyundai.com </p>
                            </div>
                        </a>
                    </div>
                    <div class="row">
                        <a href="mailto:zharu@stanford.edu" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/ruohan.jpg class="figure-img img-fluid ">
                                <p class=profname> Ruohan Zhang </p>
                                <p class=institution> Stanford University </p>
                                <p class=institution> zharu@stanford.edu </p>
                                <p class=institution> (Advisory Board) </p>
                            </div>
                        </a>
                        <a href="mailto:jmf1@cs.cmu.edu" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/jonathan.jpg class="figure-img img-fluid ">
                                <p class=profname> Jonathan Francis </p>
                                <p class=institution> Bosch Center for AI + Carnegie Mellon University </p>
                                <p class=institution> jmf1@cs.cmu.edu </p>
                                <p class=institution> (Advisory Board) </p>
                            </div>
                        </a>
                        <a href="mailto:ranjayk@allenai.org" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/ranjay.jpg class="figure-img img-fluid ">
                                <p class=profname> Ranjay Krishna </p>
                                <p class=institution> Allen Institute for AI (Ai2), University of Washington </p>
                                <p class=institution> ranjayk@allenai.org </p>
                                <p class=institution> (Advisory Board) </p>
                            </div>
                        </a>
                    </div>
		    <p> * — primary contact</p>
                </div>
            </div>
    </section>
    <br>
    <hr class="half-rule" />
    <!-- section id="tpc">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<span class=titlesec>Program Committee</span><br>


<div class="pc-column">
<ul>
<li>Ursula Addison</li>
<li>Harel Biggie</li>
<li>Gertjan Burghouts</li>
<li>Nicolas Chapman</li>
<li>Bingqing Chen (<b>MR</b>)</li>
<li>Jonathan Francis (<b>Ch</b>)</li>
<li>Ruihan Gao</li>
<li>Nikolaos Gkanatsios</li>
<li>Weiwei Gu</li>
<li>Binghao Huang</li>
<li>Nathan Hughes</li>
<li>Hanxiao Jiang</li>
<li>Nikhil Keetha</li>
<li>Anne Kemmeren</li>

</ul>
</div>

<div class="pc-column" style="margin:0 30px 0 0;">
<ul>
<li>Seungchan Kim</li>
<li>N. Suresh K. Kondepudi</li>
<li>Seungjae Lee</li>
<li>Tabitha Lee (<b>TR</b>)</li>
<li>Qiang Li (<b>ER</b>)</li>
<li>Zhixuan Liu</li>
<li>Aaron Lohner</li>
<li>Xiaopeng Lu</li>
<li>Dominic Maggio</li>
<li>Nicolas Marticorena Vidal</li>
<li>Andrew Melnik</li>
<li>Marius Memmel</li>
<li>Mark Mints</li>
<li>Daniel Omeiza (<b>TR</b>)</li>
</ul>
</div>

<div class="pc-column">
<ul>
<li>Alessandro Oltramari</li>
<li>Karthik Paga</li>
<li>Maithili Patel</li>
<li>Sarvesh Patil</li>
<li>Andrey Rudenko</li>
<li>Ateeq Sharfuddin</li>
<li>Roykrong Sukkerd (<b>TR</b>)</li>
<li>Shivam Vats</li>
<li>Rui Wang (<b>TR</b>)</li>
<li>Ho-Hsiang Wu (<b>MR</b>)</li>
<li>Yaqi Xie</li>
<li>Junjie Ye</li>
<li>Jiarui Zhang</li>
<li>Yufei Zhu</li>
</ul>
</div>
<br>
<br>
<b>ER</b> — <i>Recognises PC member who served as an Emergency Reviewer.</i><br>
<b>TR</b> — <i>Recognises PC member who, according to Workshop Chairs' ratings, ranked in the top 10% of all Reviewers.</i><br>
<b>MR</b> — <i>Recognises PC member who provided their services as a Meta-Reviewer.</i><br>
<b>Ch</b> — <i>Paper Track Chair.</i>
			    
        	    </div>
		</div>
		</div>
	</section> -->
    <!-- hr class="half-rule"/> -->
    <section id="acknowledgement">
        <div class="container">
            <div class="row">
		    <span class="titlesec">Acknowledgement</span>
    <div style="display: flex; justify-content: space-between; align-items: center;">
	    <img src="https://vib.ai.uni-bremen.de/img/fame_logo_rgb.jpg" style="width: 32%; height: auto;">
	    <img src="https://www.eurobin-project.eu/images/euROBIN_img/Ai%20noe_euROBIN_logo.png" style="width: 32%; height: auto;">
	    <img src="https://soc.kuleuven.be/lines/euexact/images/eu-logo.jpg/@@images/image.jpeg" style="width: 32%; height: auto;">
    </div>	    
		    <span style="justify-content: left; text-align:left;"> This workshop is supported by the Research Initiative FAME (Future-oriented cognitive Action Modelling Engine) and the European Network of Excellence Centers in Robotics <a href="https://www.eurobin-project.eu">euROBIN</a>.</span>
            </div>
        </div>
    </section>
<hr class="half-rule" />
<section id="contact">
        <div class="container">
            <div class="row">
		    <span class="titlesec">Contact and Information</span>
    <div style="display: flex; justify-content: space-between; align-items: center;">
	    <img src="https://semrob.github.io/images/semrob_2025_logo.png" style="width: 32%; height: auto;">
    </div>	    
		    <span style="justify-content: left; text-align:left;"><br> Direct questions to <a href="mailto:semrob.workshop+general@gmail.com">semrob.workshop+general@gmail.com</a>. Subscribe to our <a target="_blank" href="https://mailchi.mp/07f8ab4c1c65/semrob-workshop">mailing list</a> to stay updated.</span>
            </div>
        </div>
</section>	
	
	<!-- <hr class="half-rule"/>
	<section id="sponsors">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
		<span class=titlesec>Sponsors</span><br>
				<img  src=../images/sponsors/nvidia.png style="width:400px;height:220px;">
	    </div>
		</div>
		</div>
	</section> -->
    <!-- Footer -->
    <!-- Bootstrap core JavaScript -->
    <!-- 	<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
	<script src="vendor/jquery/jquery.min.js"></script>
	<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
	<!-- Plugin JavaScript -->
    <!-- <script src="vendor/jquery-easing/jquery.easing.min.js"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"> </script>
    <!-- Custom JavaScript for this theme -->
    <script src="js/scrolling-nav.js"></script>
</body>

</html>

