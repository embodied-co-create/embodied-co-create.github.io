<!DOCTYPE html>
<html lang="en">

<head>
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap" rel="stylesheet">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>RSS SemRob 2025</title>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <!-- Custom styles for this template -->
    <link href="./css/scrolling-nav.css" rel="stylesheet">
    <link href="./css/style.css" rel="author stylesheet">
    <style>
        .pc-column {
		    width:270px;
		    display:inline-block;
		    vertical-align: top;
		}
		
		.pc_list_item {
		    display:inline-block;
		    width:200px;
		}
	
	</style>
</head>

<body id="page-top">
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light" id="mainNav">
        <div class="container bar-container">
            <a class="title-head" href="#page-top">RSS SemRob 2025</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#about">About</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#speakers">Speakers</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#schedule">Schedule</a>
                    </li>
                    <!-- <li class="nav-item">
                    		    <a class="nav-link js-scroll-trigger" href="#callpapers">Call for Papers </a>
				</li> -->
                    <!--li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#papers">Accepted Papers</a>
		    		</li> -->
                    <!-- <li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#faq">FAQ</a>
				</li> -->
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#organizers">Organizers</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="/2024.html">2024</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <header class="headercontainer bg-primary text-white" style="padding: 0%; max-height: none; ">
        <div style="background-color: rgba(160,160,160,0.0)" class="text-center">
            <div style="padding-bottom: 6%; padding-top: 6%; background-image: url('./images/banner_full.png'); background-size: cover; background-position: center">
                <div class="container titlebox" ; style="display: inline-block; background-color:rgba(0,0,0, 0.7); width:auto;">
                    <p style="text-align: center; margin-bottom: 2" class="title">2nd Workshop on <b></b>Semantic Reasoning and Goal Understanding in Robotics</b> (SemRob)</p>
                    <p style="text-align: center; margin-bottom: 0" class="subtitle">Robotics Science and Systems Conference (RSS 2025) - June 21 - Los Angeles, USA</p>
                    <!-- p style="text-align: center; margin-bottom: 0" class="subtitle"><a href="https://cmu.zoom.us/j/96721732398?pwd=IjXlfZA9Nveip8RP0IrUGP080NaCsA.1" target="_blank">[<b>Zoom</b>]</a> <a href="https://app.sli.do/event/2YZ5kgotSaPWcEEgSiYS6H" target="_blank">[<b>Submit Speaker/Debate Questions</b>]</a></p -->
                </div>
            </div>
        </div>
    </header>
    <hr class="half-rule" />
    <section id="about" style="padding:70px 0 0 0">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <!-- div style="justify-content: center; text-align:center;"><img src="https://semrob.github.io/images/semrob_2024_logo.png" width="700px"></div-->
                    <span class=titlesec>About</span>
                    <br>
                    <span>
                        Semantic understanding of the world is essential for robots to make safe and informed decisions, to adapt to changing environmental conditions, and to enable efficient interactions with other agents. In pursuit of semantic understanding, agents must be able to (i) interpret and represent high-level goals, agnostic of their physical morphology and despite irrelevant aspects of their environments; they must be able to (ii) reason, i.e., to extract abstract concepts from observations in the real-world, logically manipulate these concepts, then leverage the results for inference on downstream tasks; and they must be able to (iii) execute morphology-, environment-, and socially-appropriate behaviors towards those high-level goals. </b>
                        </br>
                        </br>
                        Despite substantial recent advancements in the use of pre-trained, large-capacity models (i.e., foundation models) for difficult robotics problems, methods still struggle in the face of several practical challenges that relate to real-world deployment, e.g., cross-domain generalization, adaptation to dynamic and human-shared environments, and lifelong operation in open-world contexts. This workshop intends to sponsor discussion of new hybrid methodologies—those that combine representations from foundation models with modeling mechanisms that may prove useful for semantic reasoning and abstract goal understanding, including neural memory mechanisms, procedural modules (e.g., cognitive architectures), neuro-symbolic representations (e.g., knowledge/scene graph embeddings), chain-of-thought reasoning mechanisms, robot skill primitives and their composition, 3D scene representations (e.g., NeRFs), etc.
                        </br>
                        </br>
                        <i>Intended audience.</i> We aim to bring together engineers, researchers, and practitioners from different communities to enable avenues for interdisciplinary research on methods that could facilitate the deployment of semantics-aware and generalizable embodied agents in unstructured and dynamic real world environments. In addition to the organizers, the presenters, panelists, and technical program committee are drawn from the following (sub-)communities: Robot Learning, Embodied AI, Planning + Controls, Cognitive Robotics, Neuro-Symbolism, Natural Language Processing, Computer Vision, and Multimodal Machine Learning. We likewise intend to attract an audience from these diverse sub-communities to contribute to compelling discussions.
                    </span>
                </div>
            </div>
        </div>
    </section>
    <section id="event">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class=titlesec>Event Information</span>
                    <span>
                        This is a primarily in-person workshop, held at the <i>2025 Robotics Science and Systems conference</i> (RSS), in Los Angeles, United States on <b>21 June 2025</b>, starting at <b>14:00 PT</b>.
                        <!--
			</br>
			</br>
			The room location is <a href="https://esviewer.tudelft.nl/space/28/" target="_blank"><b>Lecture Hall D</b></a>, in the <a href="https://map.tudelftcampus.nl/poi/aula-conference-centre/" target="_blank"><b>Aula Conference Centre</b></a>, at TU Delft. You might find the <a href="https://roboticsconference.org/attending/travel/" target="_blank">travel information</a> on the RSS 2024 website helpful.
			</br>
			</br>
			Poster session location: outside the lower right-side doors of <b>Lecture Hall D</b>.
			</br>
			</br>
			RSS SemRob 2024 is <b>Workshop #28</b>, on the official RSS schedule.
			-->
                    </span>
                </div>
            </div>
        </div>
    </section>
    <hr class="half-rule" />
    <section id="speakers">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class=titlesec> Speakers and Panelists</span><br>
                    <div class="row">
                        <a href="https://jessethomason.com/" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/jesse.png" class="figure-img img-fluid ">
                                <p class=profname> Jesse Thomason </p>
                                <p class=institution>University of Southern California</p>
                            </div>
                        </a>
                        <a href="https://dorsa.fyi" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/dorsa.png" class="figure-img img-fluid ">
                                <p class=profname> Dorsa Sadigh </p>
                                <p class=institution> Stanford University </p>
                            </div>
                        </a>
                        <a href="https://talkingtorobots.com/yonatanbisk.html" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src='./images/speakers/bisk.jpg' class="figure-img img-fluid ">
                                <p class=profname> Yonatan Bisk </p>
                                <p class=institution> Carnegie Mellon University </p>
                            </div>
                        </a>
                        </a>
                        <a href="https://tedxiao.me/" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src='./images/speakers/ted.png' class="figure-img img-fluid ">
                                <p class=profname> Ted Xiao </p>
                                <p class=institution>Google DeepMind</p>
                            </div>
                        </a>
                    </div>
                    <div class="row">
                        <a href="https://msavva.github.io" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/manolis.png" class="figure-img img-fluid ">
                                <p class=profname> Manolis Savva </p>
                                <p class=institution> Simon Fraser University </p>
                            </div>
                        </a>
                        <a href="https://www.lerrelpinto.com" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/lerrel.png" class="figure-img img-fluid ">
                                <p class=profname> Lerrel Pinto </p>
                                <p class=institution> New York University </p>
                            </div>
                        </a>
                        <a href="https://www.user.tu-berlin.de/mtoussai" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/marc.png" class="figure-img img-fluid ">
                                <p class=profname> Marc Toussaint </p>
                                <p class=institution> Technical University of Berlin </p>
                            </div>
                        </a>
                    </div>
                </div>
            </div>
    </section>
    <hr class="half-rule" />
    <section class="">
        <div class="container" id="schedule">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class="titlesec"><span></span>Schedule</span>
                    <br><br>
                    <table class="table table-striped">
                        <tbody>
                            <tr>
                                <th style="width: 21%"> Time </th>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 14:00
                                    <td />
                                <td> Organizers <br> <b> Introductory Remarks </b> </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 14:10
                                    <td />
                                <td> Keynote 1: Jesse Thomason <br> <b> Title TBD </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstractmark" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstractmark" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        Abstract TBD
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 14:30
                                    <td />
                                <td> Keynote 2: Yonatan Bisk <br> <b> Semantics? Reasoning? Can define either of those terms? </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstractjacob" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstractjacob" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        In this talk I'll discuss some recent work on language conditioned robotics, but I might also choose to spend time questioning the basic assumptions of all of our work, and if we're all misguided about the important questions in robotics.
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 14:50
                                    <td />
                                <td> Keynote 3: Dorsa Sadigh <br> <b> Title TBD </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstractlerrel" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstractlerrel" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        Abstract TBD
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 15:10
                                    <td />
                                <td> Spotlight Talks.</td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 15:30
                                    <td />
                                <td> Coffee Break, Socializing, Posters</td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 16:20
                                    <td />
                                <td> Keynote 4: Ted Xiao <br> <b> Full-stack Robotics Foundation Models: From Embodied Reasoning to Dexterity </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstractmatthew" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstractmatthew" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        Abstract TBD
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 16:40
                                    <td />
                                <td> Keynote 5: Manolis Savva <br> <b> Towards Realistic & Interactive 3D Simulation for Embodied AI </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstractlerrel" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstractlerrel" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        3D simulators are increasingly being used to develop and evaluate "embodied AI" (agents perceiving and acting in realistic environments). Much of the prior work in this space has treated simulators as "black boxes" within which learning algorithms are to be deployed. However, the system characteristics of the simulation platforms themselves and the datasets that are used with these platforms both greatly impact the feasibility and the outcomes of experiments involving simulation. In this talk, I will describe several recent projects that outline emerging challenges and opportunities in the development of 3D simulation for embodied AI.
                                        Bio: Manolis Savva is an Associate Professor at Simon Fraser University, and a Canada Research Chair in Computer Graphics. His research focuses on analysis, organization and generation of 3D content. Prior to his current position he was a visiting researcher at Facebook AI Research, and a postdoctoral researcher at Princeton University. He received his Ph.D. from Stanford University under the supervision of Pat Hanrahan. His work has been recognized through several awards including an ACM UIST notable paper award (ReVision), an ICCV best paper nomination (Habitat), two SGP dataset awards (ShapeNet, SGP 2018; ScanNet, SGP 2020), the 2022 Graphics Interface early career researcher award, and an ICLR 2023 outstanding paper award (Emergence of Maps).
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 17:00
                                    <td />
                                <td> Keynote 6: Lerrel Pinto <br> <b> On Building General-Purpose Home Robots </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstractlerrel" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstractlerrel" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        The concept of a "generalist machine" in homes — a domestic assistant that can adapt and learn from our needs, all while remaining cost-effective — has long been a goal in robotics that has been steadily pursued for decades. In this talk, I will present our recent efforts towards building such capable home robots. First, I will discuss how large, pretrained vision-language models can induce strong priors for mobile manipulation tasks like pick-and-drop. But pretrained models can only take us so far. To scale beyond basic picking, we will need systems and algorithms to rapidly learn new skills. This requires creating new tools to collect data, improving representations of the visual world, and enabling trial-and-error learning during deployment. While much of the work presented focuses on two-fingered hands, I will briefly introduce learning approaches for multi-fingered hands which support more dexterous behaviors and rich touch sensing combined with vision. Finally, I will outline unsolved problems that were not obvious initially, which, when solved, will bring us closer to general-purpose home robots.
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 17:20
                                    <td />
                                <td> Keynote 7: Marc Toussaint <br> <b> Title TBD </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstractseven" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstractseven" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        Abstract TBD
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 17:40
                                    <td />
                                <td> Debate: Implicit/Data-emergent Reasoning Capabilities versus Explicit Reasoning Mechanisms? <br> <b> Panelists: Jesse Thomason, Dorsa Sadigh, Ted Xiao, Manolis Savva, Lerrel Pinto, Yonatan Bisk, Marc Toussaint</b> </td>
                            </tr>
                            <tr>
                                <td style="width: 21%">
                                    18:25
                                    <td />
                                <td> Organizers <br> <b> Closing Remarks </b>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </section>
    <br>
    <hr class="half-rule" />
    <section id="callpapers" style="padding: 70px 0 50px 0;">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class=titlesec>Call for Papers</span><br>
                    <h5 style="font-weight: bold"> Targeted Topics </h5>
                    In addition to the <a target="_blank" href="https://roboticsconference.org/information/cfp/">RSS subject areas</a>, we especially invite paper submissions on various topics, including (but not limited to):
                    <br><br>
                    <ul>
                        <li>Neural architectures leveraging demonstrations as prompts</li>
                        <li>Goal understanding through few-shot demonstrations</li>
                        <li>Novel abstractions, representations, and mechanisms for few-shot learning</li>
                        <li>Retrieval-augmentation mechanisms used in learning and task-execution</li>
                        <li>Agentic frameworks for failure reasoning, self-guidance, test-time adaptation, etc.</li>
                        <li>LLM-based action models for robot control; large action models</li>
                        <li>Semantic representations for generalizable policy learning</li>
                        <li>World models used for reasoning and optimization</li>
                    </ul>
                    <h5 style="font-weight: bold"> Submission Guidelines </h5>
                    RSS SemRob 2025 suggests <b>4+N or 8+N paper length</b> formats — i.e., 4 or 8 pages of main content with unlimited additional pages for references, appendices, etc. However, like RSS 2025, we impose no strict page length requirements on submissions; we trust that authors will recognize that respecting reviewers’ time is helpful to the evaluation of their work.<br><br>
                    Submissions are handled through CMT: <a href="https://cmt3.research.microsoft.com/SEMROB2025" target="_blank">https://cmt3.research.microsoft.com/SEMROB2025</a><br>
		    <p style="color: #AAAAAA; size:10px"><i>(Required acknowledgement: the Microsoft CMT service was used for managing the peer-reviewing process for this conference. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.)</i></p><br>
                    We will accept the official <a target="_blank" href="https://roboticsconference.org/docs/paper-template-latex.tar.gz">LaTeX</a> or <a target="_blank" href="https://roboticsconference.org/docs/paper-template-word.zip">Word</a> paper templates, provided by RSS 2025.
                    <br><br>
                    Our review process will be <b>double-blind</b>, following the RSS paper submission policy for Science/Systems papers.
                    <br><br>
                    All accepted papers will be invited for poster presentations; the highest-rated papers, according to the Technical Program Committee, will be given spotlight presentations. Accepted papers will be made available online on this workshop website as <b>non-archival</b> reports, allowing authors to also submit their works to future conferences or journals. We will highlight the Best Paper Award during the closing remarks at the workshop event.<br><br>
                    <h5 style="font-weight: bold"> Important Dates </h5>
                    <ul>
                        <li style="display: list-item">
                            <b>Submission deadline:</b>
                            <font color="#000000"><b>1 June 2025</b></font>, 23:59 AOE.
                        </li>
                        <li style="display: list-item">
                            <b>Author Notifications:</b> 10 June 2025, 23:59 AOE.
                        </li>
                        <li style="display: list-item">
                            <b>Camera Ready:</b> 15 June 2025, 23:59 AOE.
                        </li>
                        <li style="display: list-item">
                            <b>Workshop:</b> 21 June 2025, 14:00-18:30 CET
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        </div>
        </div>
    </section>
    <hr class="half-rule" />
    <!-- section id="papers">
		<div class="container">
			<div class="row">
			    <div class="col-md-10 mx-auto">
					<span class=titlesec>Accepted Papers</span><br>

				    	<p>Congratulations to Haoran Geng (SAGE: Bridging Semantic and Actionable Parts for Generalizable Manipulation of Articulated Objects) for winning the Best Paper Award!</p>

					<ul class="listpapers">
						<li>

							(Paper ID #2) la VIDA: Towards a Motivated Goal Reasoning Agent
							 <a class="linkpaper" href="./docs/rss_semrob2024_cr_paper2.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Ursula Addison
							</span>
						</li>
						<li>
							 (Paper ID #3) Open6DOR: Benchmarking Open-instruction 6-DoF Object Rearrangement and A VLM-based Approach
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper3.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Yufei Ding, Haoran Geng, Chaoyi Xu, Xiaomeng Fang, Jiazhao Zhang, Songlin Wei, Qiyu Dai, Zhizheng Zhang, He Wang
							</span>
						</li>
						<li>
								(Paper ID #4) SAGE: Bridging Semantic and Actionable Parts for Generalizable Manipulation of Articulated Objects
								<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper4.pdf" target="_blank"> [paper]</a>
								<a class="linkpaper" href="https://youtu.be/hLJlRt_MZEg?si=n1uTE60nN-y9pgTN" target="_blank"> [video]</a>
								<span style="color: #ff8c00">(spotlight)</span>
								<span style="color: #ff0000">(best paper award!)</span><br>
							<span class="authorname">Haoran Geng, Songlin Wei, Congyue Deng, Bokui Shen, He Wang, Leonidas Guibas</span>
						</li>
						<li>
								(Paper ID #5) Behavior Generation with Latent Actions 
								 <a class="linkpaper" href="./docs/rss_semrob2024_cr_paper5.pdf" target="_blank"> [paper]</a>
								<a class="linkpaper" href="https://youtu.be/UfFBXEeNbgk?si=MkfoBQjNCKIBuWOw" target="_blank"> [video]</a>
							<span style="color: #ff8c00">(spotlight)</span><br>
							<span class="authorname">Seungjae Lee, Yibin Wang, Haritheja Etukuru, Hyoun Jin Kim, Nur Muhammad (Mahi) Shafiullah, Lerrel Pinto</span>
						</li>
						<li>
							(Paper ID #6) Language models are robotic planners: reframing plans as goal refinement graphs
							 <a class="linkpaper" href="./docs/rss_semrob2024_cr_paper6.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Ateeq Sharfuddin, Travis Breaux
							</span>
						</li>
						<li>
							(Paper ID #7) OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper7.pdf" target="_blank"> [paper]</a>
						 <br>
							<span class="authorname">
							Peiqi Liu, yaswanth kumar Orru, Jay Vakil, Christopher Paxton, Nur Muhammad (Mahi) Shafiullah, Lerrel Pinto
							</span>
						</li>
						<li>
							(Paper ID #9) Clio: Real-time Task-Driven Open-Set 3D Scene Graphs
							  <a class="linkpaper" href="./docs/rss_semrob2024_cr_paper9.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Dominic Maggio, Yun Chang, Nathan Hughes, Matthew Trang, John Griffith, Eric Cristofalo, Carlyn Dougherty, Lukas Schmid, Luca Carlone
							</span>
						</li>


						<li>
							(Paper ID #10) Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper10.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta
							</span>
						</li>
						<li>
							(Paper ID #12) VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper12.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Haochen Zhang, Nader Zantout, Pujith Kachana, Zongyuan Wu, Ji Zhang, Wenshan Wang
							</span>
						</li>
						<li>

							(Paper ID #14) Enhancing Vision-Language Models with Scene Graphs for Traffic Accident Understanding
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper14.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Aaron Lohner, Francesco Compagno, Jonathan Francis, Alessandro Oltramari
						</span>
						</li>

						<li>

							(Paper ID #15) Which objects help me to act effectively? Reasoning about physically-grounded affordances
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper15.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Anne Kemmeren, Gertjan Burghouts, Michael van Bekkum, Wouter Meijer, Jelle van Mil
						</span>
						</li>

						<li>

							(Paper ID #17) RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper17.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Yuxuan Kuang, Junjie Ye, Haoran Geng, Jiageng Mao, Congyue Deng, Leonidas Guibas, He Wang, Yue Wang
						</span>
						</li>

						<li>

							(Paper ID #19) Dialog-based Skill and Task Learning for Robot
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper19.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Weiwei Gu, N. Suresh K. Kondepudi, Lixiao Huang, Nakul Gopalan
						</span>
						</li>

						<li>

							(Paper ID #20) Embodied AI Robot Companion for Efficient Object Handling in Bimanual Teleoperation
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper20.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Haolin Fei, Songlin Ma, Bo Xiao, Ziwei Wang
						</span>
						</li>

						<li>

							(Paper ID #21) Grounding Language Plans in Demonstrations Through Counterfactual Perturbations
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper21.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Yanwei Wang
						</span>
						</li>

						<li>

							(Paper ID #22) RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper22.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Hanxiao Jiang, Binghao Huang, Ruihai Wu, Zhuoran Li, Shubham Garg, Hooshang Nayyeri, Shenlong Wang, Yunzhu Li
						</span>
						</li>

						<li>

							(Paper ID #23) Lang2LTL-2: Grounding Spatiotemporal Navigation Commands Using Large Language and Vision-Language Models
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper23.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Jason Xinyu Liu, Ankit Shah, George Konidaris, Stefanie Tellex, David Paulius
						</span>
						</li>

						<li>

							(Paper ID #24) CogExplore: Contextual Exploration with Language Encoded Environment Representations
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper24.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Harel Biggie, Patrick Cooper, Doncey Albin, Kristen Such, Christoffer Heckman
						</span>
						</li>
						<li>
								(Paper ID #26) Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation
								<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper26.pdf" target="_blank"> [paper]</a>
								<a class="linkpaper" href="https://youtu.be/69CKzlAxUAY?si=XrdpCnQZRNkdU3gq" target="_blank"> [video]</a>
								<span style="color: #ff8c00">(spotlight)</span><br>
								<span class="authorname">Daniel Honerkamp, Martin Buechner, Fabien Despinoy, Tim Welschehold, Abhinav Valada</span>
						</li>


						<li>
							(Paper ID #27) Natural Language Can Help Bridge the Sim2Real Gap
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper27.pdf" target="_blank"> [paper]</a>
							<a class="linkpaper" href="https://youtu.be/-CwOOz1Gnks?si=slG8QZYVvMrjl5RI" target="_blank"> [video]</a>
							<span style="color: #ff8c00">(spotlight)</span><br>
							<span class="authorname">Albert Yu, Adeline Foote, Raymond Mooney, Roberto Martín-Martín</span>
						</li>
					</ul>

				</div>
			</div>
		</div>
	</section> -->
    <hr class="half-rule" />
    <section id="organizers">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class=titlesec>Organizers</span><br>
                    <div class="row">
                        <a href="#">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/melnik.png class="figure-img img-fluid ">
                                <p class=profname> <a href="https://www.linkedin.com/in/andrewmelnik">Andrew Melnik</a> </p>
                                <p class=institution> Bremen University</p>
                            </div>
                        </a>
                        <a href="#">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/jon.png class="figure-img img-fluid ">
                                <p class=profname><a href="#">Jonathan Francis</a></p>
                                <p class=institution>Bosch Center for AI; Carnegie Mellon University</p>
                            </div>
                        </a>
                        <a href="#">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/krishan.jpg class="figure-img img-fluid ">
                                <p class=profname><a href="#"> Krishan Rana </a> </p>
                                <p class=institution> QUT Centre for Robotics </p>
                            </div>
                        </a>
                    </div>
                    <div class="row">
                        <a href="#">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/mehreen.png class="figure-img img-fluid ">
                                <p class=profname><a href="#"> Mehreen Naeem </a> </p>
                                <p class=institution> Bremen University </p>
                            </div>
                        </a>
                        <a href="#">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/ishika.png class="figure-img img-fluid ">
                                <p class=profname><a href="#"> Ishika Singh </a> </p>
                                <p class=institution> University of Southern California </p>
                            </div>
                        </a>
                        <a href="#">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/siddhant.png class="figure-img img-fluid ">
                                <p class=profname><a href="#"> Siddhant Haldar </a> </p>
                                <p class=institution> New York University </p>
                            </div>
                        </a>
                    </div>
                </div>
            </div>
    </section>
    <br>
    <hr class="half-rule" />
    <!-- section id="tpc">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<span class=titlesec>Program Committee</span><br>


<div class="pc-column">
<ul>
<li>Ursula Addison</li>
<li>Harel Biggie</li>
<li>Gertjan Burghouts</li>
<li>Nicolas Chapman</li>
<li>Bingqing Chen (<b>MR</b>)</li>
<li>Jonathan Francis (<b>Ch</b>)</li>
<li>Ruihan Gao</li>
<li>Nikolaos Gkanatsios</li>
<li>Weiwei Gu</li>
<li>Binghao Huang</li>
<li>Nathan Hughes</li>
<li>Hanxiao Jiang</li>
<li>Nikhil Keetha</li>
<li>Anne Kemmeren</li>

</ul>
</div>

<div class="pc-column" style="margin:0 30px 0 0;">
<ul>
<li>Seungchan Kim</li>
<li>N. Suresh K. Kondepudi</li>
<li>Seungjae Lee</li>
<li>Tabitha Lee (<b>TR</b>)</li>
<li>Qiang Li (<b>ER</b>)</li>
<li>Zhixuan Liu</li>
<li>Aaron Lohner</li>
<li>Xiaopeng Lu</li>
<li>Dominic Maggio</li>
<li>Nicolas Marticorena Vidal</li>
<li>Andrew Melnik</li>
<li>Marius Memmel</li>
<li>Mark Mints</li>
<li>Daniel Omeiza (<b>TR</b>)</li>
</ul>
</div>

<div class="pc-column">
<ul>
<li>Alessandro Oltramari</li>
<li>Karthik Paga</li>
<li>Maithili Patel</li>
<li>Sarvesh Patil</li>
<li>Andrey Rudenko</li>
<li>Ateeq Sharfuddin</li>
<li>Roykrong Sukkerd (<b>TR</b>)</li>
<li>Shivam Vats</li>
<li>Rui Wang (<b>TR</b>)</li>
<li>Ho-Hsiang Wu (<b>MR</b>)</li>
<li>Yaqi Xie</li>
<li>Junjie Ye</li>
<li>Jiarui Zhang</li>
<li>Yufei Zhu</li>
</ul>
</div>
<br>
<br>
<b>ER</b> — <i>Recognises PC member who served as an Emergency Reviewer.</i><br>
<b>TR</b> — <i>Recognises PC member who, according to Workshop Chairs' ratings, ranked in the top 10% of all Reviewers.</i><br>
<b>MR</b> — <i>Recognises PC member who provided their services as a Meta-Reviewer.</i><br>
<b>Ch</b> — <i>Paper Track Chair.</i>
			    
        	    </div>
		</div>
		</div>
	</section> -->
    <!-- hr class="half-rule"/> -->
    <section id="contact">
        <div class="container">
            <div class="row">
                <div display="inline"><img src="https://semrob.github.io/images/semrob_2025_logo.png" width="400px"></div>
                <div style="display:inline; width:520px; align-items: center; vertical-align: bottom; padding:20px 0 0 15px;">
                    <span class="titlesec">Contact and Information</span><br>
                    <span style="justify-content: left; text-align:left;">Direct questions to <a href="mailto:semrob.workshop+general@gmail.com">semrob.workshop+general@gmail.com</a>.</span>
                    <br><br>
                    <span style="justify-content: left; text-align:left;">Subscribe to our <a target="_blank" href="https://mailchi.mp/07f8ab4c1c65/semrob-workshop">mailing list</a> to stay updated.</span>
                </div>
            </div>
        </div>
    </section>
    <!-- <hr class="half-rule"/>
	<section id="sponsors">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
		<span class=titlesec>Sponsors</span><br>
				<img  src=../images/sponsors/nvidia.png style="width:400px;height:220px;">
	    </div>
		</div>
		</div>
	</section> -->
    <!-- Footer -->
    <!-- Bootstrap core JavaScript -->
    <!-- 	<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
	<script src="vendor/jquery/jquery.min.js"></script>
	<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
	<!-- Plugin JavaScript -->
    <!-- <script src="vendor/jquery-easing/jquery.easing.min.js"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"> </script>
    <!-- Custom JavaScript for this theme -->
    <script src="js/scrolling-nav.js"></script>
</body>

</html>
