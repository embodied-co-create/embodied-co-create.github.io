<!DOCTYPE html>
<html lang="en">

<head>
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap" rel="stylesheet">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>RSS SemRob 2025</title>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <!-- Custom styles for this template -->
    <link href="./css/scrolling-nav.css" rel="stylesheet">
    <link href="./css/style.css" rel="author stylesheet">
    <style>
        .pc-column {
		    width:270px;
		    display:inline-block;
		    vertical-align: top;
		}
		
		.pc_list_item {
		    display:inline-block;
		    width:200px;
		}
	
	</style>
</head>

<body id="page-top">
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light" id="mainNav">
        <div class="container bar-container">
            <a class="title-head" href="#page-top">RSS SemRob 2025</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#about">About</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#speakers">Speakers</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#schedule">Schedule</a>
                    </li>
                    <!-- <li class="nav-item">
                    		    <a class="nav-link js-scroll-trigger" href="#callpapers">Call for Papers </a>
				</li> -->
                    <!--li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#papers">Accepted Papers</a>
		    		</li> -->
                    <!-- <li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#faq">FAQ</a>
				</li> -->
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#organizers">Organizers</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="/2024.html">2024</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <header class="headercontainer bg-primary text-white" style="padding: 0%; max-height: none; ">
        <div style="background-color: rgba(160,160,160,0.0)" class="text-center">
            <div style="padding-bottom: 6%; padding-top: 6%; background-image: url('./images/banner_full.png'); background-size: cover; background-position: center">
                <div class="container titlebox" ; style="display: inline-block; background-color:rgba(0,0,0, 0.7); width:auto;">
                    <p style="text-align: center; margin-bottom: 2" class="title">2nd Workshop on <b></b>Semantic Reasoning and Goal Understanding in Robotics</b> (SemRob)</p>
                    <p style="text-align: center; margin-bottom: 0" class="subtitle"><a href="https://roboticsconference.org" target="_blank">Robotics Science and Systems Conference (RSS 2025)</a> <br> June 21 - Los Angeles, USA</p>
                    <!-- p style="text-align: center; margin-bottom: 0" class="subtitle"><a href="https://cmu.zoom.us/j/96721732398?pwd=IjXlfZA9Nveip8RP0IrUGP080NaCsA.1" target="_blank">[<b>Zoom</b>]</a> <a href="https://app.sli.do/event/2YZ5kgotSaPWcEEgSiYS6H" target="_blank">[<b>Submit Speaker/Debate Questions</b>]</a></p -->
                </div>
            </div>
        </div>
    </header>
    <hr class="half-rule" />
    <section id="about" style="padding:70px 0 0 0">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <!-- div style="justify-content: center; text-align:center;"><img src="https://semrob.github.io/images/semrob_2024_logo.png" width="700px"></div-->
                    <span class=titlesec>About</span>
                    <br>
                    <span>
                        Semantic understanding of the world is essential for robots to make safe and informed decisions, to adapt to changing environmental conditions, and to enable efficient interactions with other agents. In pursuit of semantic understanding, agents must be able to (i) interpret and represent high-level goals, agnostic of their physical morphology and despite irrelevant aspects of their environments; they must be able to (ii) reason, i.e., to extract abstract concepts from observations in the real-world, logically manipulate these concepts, then leverage the results for inference on downstream tasks; and they must be able to (iii) execute morphology-, environment-, and socially-appropriate behaviors towards those high-level goals. </b>
                        </br>
                        </br>
                        Despite substantial recent advancements in the use of pre-trained, large-capacity models (i.e., foundation models) for difficult robotics problems, methods still struggle in the face of several practical challenges that relate to real-world deployment, e.g., cross-domain generalization, adaptation to dynamic and human-shared environments, and lifelong operation in open-world contexts. This workshop intends to sponsor discussion of new hybrid methodologiesâ€”those that combine representations from foundation models with modeling mechanisms that may prove useful for semantic reasoning and abstract goal understanding, including neural memory mechanisms, procedural modules (e.g., cognitive architectures), neuro-symbolic representations (e.g., knowledge/scene graph embeddings), chain-of-thought reasoning mechanisms, robot skill primitives and their composition, <a href="https://arxiv.org/abs/2504.13159" target="_blank">3D scene representations (e.g., 3DGS)</a>, etc.
                        </br>
                        </br>
                        <i>Intended audience.</i> We aim to bring together engineers, researchers, and practitioners from different communities to enable avenues for interdisciplinary research on methods that could facilitate the deployment of semantics-aware and generalizable embodied agents in unstructured and dynamic real world environments. In addition to the organizers, the presenters, panelists, and technical program committee are drawn from the following communities: Robot Learning, Embodied AI, Planning + Controls, Cognitive Robotics, Neuro-Symbolism, Natural Language Processing, Multimodal Machine Learning, Computer Vision, and <a href="https://arxiv.org/abs/2504.13159" target="_blank">Digital Twins</a>. We likewise intend to attract an audience from these diverse sub-communities to contribute to compelling discussions.
                    </span>
                </div>
            </div>
        </div>
    </section>
    <section id="event">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class=titlesec>Event Information</span>
                    <span>
                        This is a primarily in-person workshop, held at the <i>2025 Robotics Science and Systems conference</i> (RSS), in Los Angeles, United States on <b>21 June 2025</b>, starting at <b>8:30am PT</b>.
                        <!--
			</br>
			</br>
			The room location is <a href="https://esviewer.tudelft.nl/space/28/" target="_blank"><b>Lecture Hall D</b></a>, in the <a href="https://map.tudelftcampus.nl/poi/aula-conference-centre/" target="_blank"><b>Aula Conference Centre</b></a>, at TU Delft. You might find the <a href="https://roboticsconference.org/attending/travel/" target="_blank">travel information</a> on the RSS 2024 website helpful.
			</br>
			</br>
			Poster session location: outside the lower right-side doors of <b>Lecture Hall D</b>.
			</br>
			</br>
			RSS SemRob 2024 is <b>Workshop #28</b>, on the official RSS schedule.
			-->
                    </span>
                </div>
            </div>
        </div>
    </section>
    <hr class="half-rule" />
    <section id="speakers">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class=titlesec> Speakers and Panelists</span><br>
                    <div class="row">
                        <a href="https://jessethomason.com/" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/jesse.png" class="figure-img img-fluid ">
                                <p class=profname> Jesse Thomason </p>
                                <p class=institution>University of Southern California</p>
                            </div>
                        </a>
                        <a href="https://dorsa.fyi" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/dorsa.png" class="figure-img img-fluid ">
                                <p class=profname> Dorsa Sadigh </p>
                                <p class=institution> Stanford University </p>
                            </div>
                        </a>
                        <a href="https://talkingtorobots.com/yonatanbisk.html" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src='./images/speakers/bisk.jpg' class="figure-img img-fluid ">
                                <p class=profname> Yonatan Bisk </p>
                                <p class=institution> Carnegie Mellon University </p>
                            </div>
                        </a>
                        </a>
                        <a href="https://tedxiao.me/" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src='./images/speakers/ted.png' class="figure-img img-fluid ">
                                <p class=profname> Ted Xiao </p>
                                <p class=institution>Google DeepMind</p>
                            </div>
                        </a>
                    </div>
                    <div class="row">
                        <a href="https://msavva.github.io" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/manolis.png" class="figure-img img-fluid ">
                                <p class=profname> Manolis Savva </p>
                                <p class=institution> Simon Fraser University </p>
                            </div>
                        </a>
                        <a href="https://www.lerrelpinto.com" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/lerrel.png" class="figure-img img-fluid ">
                                <p class=profname> Lerrel Pinto </p>
                                <p class=institution> New York University </p>
                            </div>
                        </a>
                        <a href="https://ai.uni-bremen.de/team/benjamin_alt" target="_blank">
                            <div class="profpic speaker xlarge-1 columns">
                                <img src="./images/speakers/benjamin.jpeg" class="figure-img img-fluid ">
                                <p class=profname> Benjamin Alt </p>
                                <p class=institution> Bremen University </p>
                            </div>
                        </a>
		    </div>
                </div>
            </div>
    </section>
    <hr class="half-rule" />
    <section class="">
        <div class="container" id="schedule">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class="titlesec"><span></span>Schedule</span>
                    <br><br>
                    <table class="table table-striped">
                        <tbody>
                            <tr>
                                <th style="width: 21%"> Time </th>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 08:30
                                    <td />
                                <td> Organizers <br> <b> Introductory Remarks </b> </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 08:40
                                    <td />
                                <td> Keynote 1: Jesse Thomason <br> <b> Embracing Language as Grounded Communication </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstract-JT" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstract-JT" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        Language is not text data, it is a human medium for communication. The larger part of the natural language processing (NLP) community has doubled down on treating digital text as a sufficient approximation of language, scaling datasets and corresponding models to fit that text. I have argued that experience in the world grounds language, tying it to objects, actions, and concepts. In fact, I believe that language carries meaning only when considered alongside that world, and that the zeitgeist in NLP research currently misses the mark on truly interesting questions at the intersection of human language and machine computation. In this talk, Iâ€™ll highlight some of the ways my lab enables agents and robots to better understand and respond to human communication by considering the grounded context in which that communication occurs, including neurosymbolic multimodal reasoning, natural language dialogue and interaction for lifelong learning, and utilizing NLP technologies on non-text communication.
					    <br><br>
					    Keynote references:<br>
					    <a target="_blank" href="https://arxiv.org/abs/2406.02791">PSALM</a><br>
					    <a target="_blank" href="https://progprompt.github.io">ProgPrompt</a><br>
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 09:00
                                    <td />
                                <td> Keynote 2: Manolis Savva <br> <b> Towards Realistic & Interactive 3D Simulation for Embodied AI </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstract-MS" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstract-MS" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        3D simulators are increasingly being used to develop and evaluate "embodied AI" (agents perceiving and acting in realistic environments). Much of the prior work in this space has treated simulators as "black boxes" within which learning algorithms are to be deployed. However, the system characteristics of the simulation platforms themselves and the datasets that are used with these platforms both greatly impact the feasibility and the outcomes of experiments involving simulation. In this talk, I will describe several recent projects that outline emerging challenges and opportunities in the development of 3D simulation for embodied AI.
					    <br><br>
                                        Bio: Manolis Savva is an Associate Professor at Simon Fraser University, and a Canada Research Chair in Computer Graphics. His research focuses on analysis, organization and generation of 3D content. Prior to his current position he was a visiting researcher at Facebook AI Research, and a postdoctoral researcher at Princeton University. He received his Ph.D. from Stanford University under the supervision of Pat Hanrahan. His work has been recognized through several awards including an ACM UIST notable paper award (ReVision), an ICCV best paper nomination (Habitat), two SGP dataset awards (ShapeNet, SGP 2018; ScanNet, SGP 2020), the 2022 Graphics Interface early career researcher award, and an ICLR 2023 outstanding paper award (Emergence of Maps).
					    <br><br>
					    Keynote references:<br>
					    <a target="_blank" href="https://3dlg-hcvc.github.io/hssd/">Habitat Synthetic Scenes Dataset (HSSD)</a><br>
					    <a target="_blank" href="https://3dlg-hcvc.github.io/smc/">SceneMotifCoder</a><br>
					    <a target="_blank" href="https://3dlg-hcvc.github.io/s2o/">S2O: Static to Openable</a><br>
					    <a target="_blank" href="https://3dlg-hcvc.github.io/cage/">CAGE: Controllable Articulation GEneration</a><br>
					    <a target="_blank" href="https://3dlg-hcvc.github.io/singapo/">SINGAPO</a><br>
                                    </div>
                                </td>
                            </tr>				
                            <tr>
                                <td style="width: 21%"> 09:20
                                    <td />
                                <td> Keynote 3: Dorsa Sadigh <br> <b> Human-Aligned Robot Learning: manipulation policies via preferences, RLHF, and VLM feedback </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstract-DS" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstract-DS" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        Abstract TBD
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 09:40
                                    <td />
                                <td> Spotlight Talks</td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 10:00
                                    <td />
                                <td> Coffee Break, Socializing, Posters</td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 10:30
                                    <td />
                                <td> Keynote 4: Benjamin Alt <br> <b> Semantic Digital Twins for Robust and Flexible Robot Behavior </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#benjamin" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="benjamin" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
					    Keynote references:<br>
					    <a target="_blank" href="https://arxiv.org/abs/2504.13159">Digital Twin Generation from Visual Data: A Survey</a><br>
					    <a target="_blank" href="https://github.com/ndrwmlnk/awesome-digital-twins">Awesome Digital Twins</a><br>
					    
                                    </div>
                                </td>
                            </tr>				
                            <tr>
                                <td style="width: 21%"> 10:40
                                    <td />
                                <td> Keynote 5: Ted Xiao <br> <b> Full-stack Robotics Foundation Models: From Embodied Reasoning to Dexterity </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstractmatthew" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstractmatthew" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        Advances in data-driven robot learning have accelerated progress towards general purpose robotic control. While improvements in Vision Language Action (VLA) models and large-scale imitation learning have enabled early multipurpose robotic foundation models, progress is often a direct reflection of the robot training dataset distribution or bespoke algorithmic adjustments. This stands in stark contrast to trends in multimodal frontier models, where capability improvements come not only from nuanced small-scale design decisions, but from properly harnessing the fundamental intelligence scaling laws of the underlying frontier model. In this talk, I will discuss how perspectives from frontier modeling can inspire and guide robotics research. In particular, I will cover how Gemini Robotics tackles robotics from a truly full-stack approach: how improving multimodal frontier model capabilities like embodied reasoning results in a generalizable, steerable, and dexterous robot foundation model.
					    <br><br>
					    Keynote references:<br>
					    <a target="_blank" href="https://deepmind.google/models/gemini-robotics/">https://deepmind.google/models/gemini-robotics/</a><br>

                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 11:00
                                    <td />
                                <td> Keynote 6: Yonatan Bisk <br> <b> Semantics? Reasoning? Can we define either of those terms? </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstract-YB" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstract-YB" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        In this talk I'll discuss some recent work on language conditioned robotics, but I might also choose to spend time questioning the basic assumptions of all of our work, and if we're all misguided about the important questions in robotics.
                                    </div>
                                </td>
                            </tr>				
                            <tr>
                                <td style="width: 21%"> 11:20
                                    <td />
                                <td> Keynote 7: Lerrel Pinto <br> <b> On Building General-Purpose Home Robots </b>
                                    <br>
                                    <a data-toggle="collapse" data-target="#abstract-LP" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                                    <div id="abstract-LP" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
                                        The concept of a "generalist machine" in homes â€” a domestic assistant that can adapt and learn from our needs, all while remaining cost-effective â€” has long been a goal in robotics that has been steadily pursued for decades. In this talk, I will present our recent efforts towards building such capable home robots. First, I will discuss how large, pretrained vision-language models can induce strong priors for mobile manipulation tasks like pick-and-drop. But pretrained models can only take us so far. To scale beyond basic picking, we will need systems and algorithms to rapidly learn new skills. This requires creating new tools to collect data, improving representations of the visual world, and enabling trial-and-error learning during deployment. While much of the work presented focuses on two-fingered hands, I will briefly introduce learning approaches for multi-fingered hands which support more dexterous behaviors and rich touch sensing combined with vision. Finally, I will outline unsolved problems that were not obvious initially, which, when solved, will bring us closer to general-purpose home robots.
					    <br><br>
					    Keynote references:<br>
					    <a target="_blank" href="https://robotutilitymodels.com/">Robot Utility Models</a><br>
					    <a target="_blank" href="https://egozero-robot.github.io/">EgoZero: Robot Learning from Smart Glasses</a><br>
					    <a target="_blank" href="https://dynamem.github.io/">DynaMem</a><br>
					    <a target="_blank" href="https://point-policy.github.io/">Point Policy</a><br>
                                    </div>
                                </td>
                            </tr>
                            <tr>
                                <td style="width: 21%"> 11:40
                                    <td />
                                <td> Debate: Implicit/Data-emergent Reasoning Capabilities versus Explicit Reasoning Mechanisms? <br> <b> Panelists: Jesse Thomason, Ted Xiao, Manolis Savva, Lerrel Pinto, Yonatan Bisk</b> </td>
                            </tr>
                            <tr>
                                <td style="width: 21%">
                                    12:30
                                    <td />
                                <td> Organizers <br> <b> Closing Remarks </b>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </section>
    <br>
    <hr class="half-rule" />
    <section id="callpapers" style="padding: 70px 0 50px 0;">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class=titlesec>Call for Papers</span><br>
                    <h5 style="font-weight: bold"> Targeted Topics </h5>
                    In addition to the <a target="_blank" href="https://roboticsconference.org/information/cfp/">RSS subject areas</a>, we especially invite paper submissions on various topics, including (but not limited to):
                    <br><br>
                    <ul>
                        <li>Neural architectures leveraging demonstrations as prompts</li>
                        <li>Goal understanding through few-shot demonstrations</li>
                        <li>Novel abstractions, representations, and mechanisms for few-shot learning</li>
                        <li>Retrieval-augmentation mechanisms used in learning and task-execution</li>
                        <li>Agentic frameworks for failure reasoning, self-guidance, test-time adaptation, etc.</li>
                        <li>LLM-based action models for robot control; large action models</li>
                        <li>Semantic representations for generalizable policy learning</li>
                        <li>World models used for reasoning and optimization</li>
                    </ul>
                    <h5 style="font-weight: bold"> Submission Guidelines </h5>
                    RSS SemRob 2025 suggests <b>4+N or 8+N paper length</b> formats â€” i.e., 4 or 8 pages of main content with unlimited additional pages for references, appendices, etc. However, like RSS 2025, we impose no strict page length requirements on submissions; we trust that authors will recognize that respecting reviewersâ€™ time is helpful to the evaluation of their work.<br><br>
                    Submissions are handled through CMT: <a href="https://cmt3.research.microsoft.com/SEMROB2025" target="_blank">https://cmt3.research.microsoft.com/SEMROB2025</a><br>
		    <p style="color: #AAAAAA; size:10px"><i>(Required acknowledgement: the Microsoft CMT service was used for managing the peer-reviewing process for this conference. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.)</i></p><br>
                    We will accept the official <a target="_blank" href="https://roboticsconference.org/docs/paper-template-latex.tar.gz">LaTeX</a> or <a target="_blank" href="https://roboticsconference.org/docs/paper-template-word.zip">Word</a> paper templates, provided by RSS 2025.
                    <br><br>
                    Our review process will be <b>double-blind</b>, following the RSS paper submission policy for Science/Systems papers.
                    <br><br>
                    All accepted papers will be invited for poster presentations; the highest-rated papers, according to the Technical Program Committee, will be given spotlight presentations. Accepted papers will be made available online on this workshop website as <b>non-archival</b> reports, allowing authors to also submit their works to future conferences or journals. We will highlight the Best Paper Award during the closing remarks at the workshop event.<br><br>
                    <h5 style="font-weight: bold"> Important Dates </h5>
                    <ul>
                        <li style="display: list-item">
                            <b>Submission deadline:</b>
                            <s><font color="#000000"><b>1 June 2025</b></font>, 23:59 AOE.</s>
                        </li>
                        <li style="display: list-item">
                            <s><b>Author Notifications:</b> 10 June 2025.</s>s>
                        </li>
                        <li style="display: list-item">
                            <b>Camera Ready:</b> 15 June 2025, 23:59 AOE.
                        </li>
                        <li style="display: list-item">
                            <b>Workshop:</b> 21 June 2025, 08:30-12:30 PT
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        </div>
        </div>
    </section>
    <hr class="half-rule" />
    <section id="papers">
		<div class="container">
			<div class="row">
			    <div class="col-md-10 mx-auto">
					<span class=titlesec>Accepted Papers</span><br>

				    	<!-- p>Congratulations to Haoran Geng (SAGE: Bridging Semantic and Actionable Parts for Generalizable Manipulation of Articulated Objects) for winning the Best Paper Award!</p -->

					<ul class="listpapers">
						<li>

							(Paper ID #1) Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models
							 <!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper1.pdf" target="_blank"> [paper]</a -->
							<span style="color: #ff8c00">(spotlight)</span>
							<br>
							<span class="authorname">
							Xiaoyang Shi; Brian Ichter; Michael Equi; Liyiming Ke; Karl Pertsch; Quan Vuong; James Tanner; Anna Walling; Haohuan Wang; Niccolo Fusai; Adrian Li-Bell; Danny Driess; Lachy Groom; Sergey Levine; Chelsea Finn
							</span>
						</li>
						<li>
							(Paper ID #2) VERDI: VLM-Embedded Reasoning for Autonomous Driving
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper2.pdf" target="_blank"> [paper]</a -->
							<br>
							<span class="authorname">
							Zhiting Mei; Bowen Feng; Baiang Li; Julian Ost; Roger Girgis; Anirudha Majumdar; Felix Heide
							</span>
						</li>
						<li>
							(Paper ID #3) STRIVE: Structured Representation Integrating VLM Reasoning for Efficient Object Navigation
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper3.pdf" target="_blank"> [paper]</a -->
							<!-- a class="linkpaper" href="https://youtu.be/hLJlRt_MZEg?si=n1uTE60nN-y9pgTN" target="_blank"> [video]</a -->
							<span class="authorname">
								Haokun Zhu; Zongtai Li; Zhixuan Liu; Wenshan Wang; Ji Zhang; Jonathan Francis; Jean Oh
							</span>
						</li>
						<li>
							(Paper ID #4) Flexible Multitask Learning with Factorized Diffusion Policy
							 <!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper4.pdf" target="_blank"> [paper]</a -->
							<!-- a class="linkpaper" href="https://youtu.be/UfFBXEeNbgk?si=MkfoBQjNCKIBuWOw" target="_blank"> [video]</a -->
							<span style="color: #ff8c00">(spotlight)</span><br>
							<span class="authorname">
								Chaoqi Liu
							</span>
						</li>
						<li>
							(Paper ID #5) RayFronts: Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration
							 <!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper5.pdf" target="_blank"> [paper]</a -->
							<br>
							<span class="authorname">
							Omar Alama; Avigyan Bhattacharya; Haoyang He; Seungchan Kim; Yuheng Qiu; Wenshan Wang; Cherie Ho; Nikhil Keetha; Sebastian Scherer
							</span>
						</li>
						<li>
							(Paper ID #6) EgoZero: Robot Learning from Smart Glasses
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper6.pdf" target="_blank"> [paper]</a -->
						 	<br>
							<span class="authorname">
							Vincent Liu*; Ademi Adeniji*; Haotian Zhan*; Siddhant Haldar; Raunaq Bhirangi; Pieter Abbeel; Lerrel Pinto
							</span>
						</li>
						<li>
							(Paper ID #7) Touch begins where vision ends: Generalizable policies for contact-rich manipulation
							  <!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper7.pdf" target="_blank"> [paper]</a -->
							<br>
							<span class="authorname">
							Zifan Zhao; Siddhant Haldar; Jinda Cui; Lerrel Pinto; Raunaq Bhirangi
							</span>
						</li>


						<li>
							(Paper ID #8) Feel the Force: Contact-Driven Learning from Humans
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper8.pdf" target="_blank"> [paper]</a -->
							<br>
							<span class="authorname">
							Ademi Adeniji; Zhuoran Chen; Vincent Liu; Venkatesh Pattabiraman; Siddhant Haldar; Raunaq Bhirangi; Pieter Abbeel; Lerrel Pinto
							</span>
						</li>
						<li>
							(Paper ID #9) IMPACT: Intelligent Motion Planning with Acceptable Contact Trajectories via Vision-Language Models
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper9.pdf" target="_blank"> [paper]</a -->
							<br>
							<span class="authorname">
							Yiyang Ling; Karan Owalekar; Oluwatobiloba Adesanya; Erdem BÄ±yÄ±k; Daniel Seita
							</span>
						</li>
						<li>

							(Paper ID #10) Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper10.pdf" target="_blank"> [paper]</a -->
							<br>
							<span class="authorname">
							Siddhant Haldar; Lerrel Pinto
						</span>
						</li>

						<li>
							(Paper ID #11) GRIM: Task-Oriented Grasping with Conditioning on Generative Examples
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper11.pdf" target="_blank"> [paper]</a -->
							<br>
							<span class="authorname">
							Shailesh Shailesh; Alok Raj; Nayan Kumar; Priya Shukla; Andrew Melnik; Michael Beetz; Gora Chand Nandi
						</span>
						</li>

						<li>

							(Paper ID #12) Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper12.pdf" target="_blank"> [paper]</a -->
							<br>
							<span class="authorname">
							Sigmund HÃ¸eg; Chaoqi Liu; Yilun Du; Olav Egeland
						</span>
						</li>

						<li>

							(Paper ID #13) WoMAP: World Models For Embodied Open-Vocabulary Object Localization
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper13.pdf" target="_blank"> [paper]</a -->
							<span style="color: #ff8c00">(spotlight)</span>
							<br>
							<span class="authorname">
							Tenny Yin; May Mei; Tao Sun; Lihan Zha; Jeremy Bao; Emily Zhou; Miyu Yamane; Ola Shorinw; Anirudha Majumdar
						</span>
						</li>

						<li>

							(Paper ID #14) Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agents
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper14.pdf" target="_blank"> [paper]</a -->
							<br>
							<span class="authorname">
							Che Rin Yu; Daewon Chae; Dabin Seo; Yoonha Jang; Sangwon Lee; Hyeongwoo IM; Jinkyu Kim
						</span>
						</li>

						<li>

							(Paper ID #15) CASPER: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper15.pdf" target="_blank"> [paper]</a -->
							<br>
							<span class="authorname">
							Huihan Liu; Rutav Shah; Shuijing Liu; Jack Pittenger; Mingyo Seo; Yuchen Cui; Yonatan Bisk; Roberto Martin-Martin; Yuke Zhu
						</span>
						</li>

						<li>

							(Paper ID #16) Grounding Language Models with Semantic Digital Twins for Robotic Planning
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper16.pdf" target="_blank"> [paper]</a -->
							<br>
							<span class="authorname">
							Mehreen Naeem; Andrew Melnik; Michael Beetz
						</span>
						</li>

						<li>

							(Paper ID #17) MotIF: Motion Instruction Fine-tuning
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper17.pdf" target="_blank"> [paper]</a -->
							<span style="color: #ff8c00">(spotlight)</span>
							<br>
							<span class="authorname">
							Minyoung Hwang; Joey Hejna; Dorsa Sadigh; Yonatan Bisk
						</span>
						</li>

						<li>
							(Paper ID #18) Mixed Initiative Dialog for Human-Robot Collaborative Mobile Manipulation
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper18.pdf" target="_blank"> [paper]</a -->
							<br>
							<span class="authorname">
							Albert Yu; Chengshu Li; Luca Macesanu; Arnav Balaji; Ruchira Ray; Ray Mooney; Roberto Roberto MartÃ­n-MartÃ­n
						</span>
						</li>
						<li>
							(Paper ID #19) GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic Guidance
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper19.pdf" target="_blank"> [paper]</a -->
							<!-- a class="linkpaper" href="https://youtu.be/69CKzlAxUAY?si=XrdpCnQZRNkdU3gq" target="_blank"> [video]</a -->
							<br>
							<span class="authorname">
								Arthur Fender Coelho Bucker; Pablo Ortega Kral; Jonathan Francis; Jean Oh
							</span>
						</li>


						<li>
							(Paper ID #20) Points2Reward: Robotic Manipulation Rewards from Just One Video
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper20.pdf" target="_blank"> [paper]</a -->
							<!-- a class="linkpaper" href="https://youtu.be/-CwOOz1Gnks?si=slG8QZYVvMrjl5RI" target="_blank"> [video]</a -->
							<br>
							<span class="authorname">
								Junyao Shi; Joshua Smith; Jianing Qian; Dinesh Jayaraman
							</span>
						</li>
						<li>
							(Paper ID #21) Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper21.pdf" target="_blank"> [paper]</a -->
							<!-- a class="linkpaper" href="https://youtu.be/-CwOOz1Gnks?si=slG8QZYVvMrjl5RI" target="_blank"> [video]</a -->
							<br>
							<span class="authorname">
								Yaru Niu; Yunzhe Zhang; Mingyang Yu; Changyi Lin; Chenhao Li; Yikai Wang; Yuxiang Yang; Wenhao Yu; Tingnan Zhang; Zhenzhen Li; Jonathan Francis; Bingqing Chen; Jie Tan; Ding Zhao
							</span>
						</li>
						<li>
							(Paper ID #22) GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering
							<!-- a class="linkpaper" href="./docs/rss_semrob2025_cr_paper22.pdf" target="_blank"> [paper]</a -->
							<!-- a class="linkpaper" href="https://youtu.be/-CwOOz1Gnks?si=slG8QZYVvMrjl5RI" target="_blank"> [video]</a -->
							<br>
							<span class="authorname">
								Blake Buchanan; Saumya Saxena; Chris Paxton; Bingqing Chen; Jonathan Francis; Narunas Vaskevicius; Luigi Palmieri; Peiqi Liu; Oliver Kroemer
							</span>
						</li>
					</ul>

				</div>
			</div>
		</div>
	</section> 
    <hr class="half-rule" />
    <section id="organizers">
        <div class="container">
            <div class="row">
                <div class="col-md-10 mx-auto">
                    <span class=titlesec>Organizers</span><br>
                    <div class="row">
                        <a href="https://scholar.google.com/citations?user=6tiiQtgAAAAJ&hl=en" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/melnik.png class="figure-img img-fluid ">
                                <p class=profname> Andrew Melnik* </p>
                                <p class=institution> Bremen University</p>
                            </div>
                        </a>
                        <a href="https://jonfranc.com" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/jon.png class="figure-img img-fluid ">
                                <p class=profname> Jonathan Francis* </p>
                                <p class=institution>Bosch Center for AI; Carnegie Mellon University</p>
                            </div>
                        </a>
                        <a href="https://krishanrana.github.io/" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/krishan.jpg class="figure-img img-fluid ">
                                <p class=profname> Krishan Rana </p>
                                <p class=institution> QUT Centre for Robotics </p>
                            </div>
                        </a>
                    </div>
                    <div class="row">
                        <a href="https://mzhao98.github.io/" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/michelle.png class="figure-img img-fluid ">
                                <p class=profname> Michelle Zhao </p>
                                <p class=institution> Carnegie Mellon University </p>
                            </div>
                        </a>
			<a href="https://ai.uni-bremen.de/team/mehreen_naeem" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/mehreen.png class="figure-img img-fluid ">
                                <p class=profname> Mehreen Naeem </p>
                                <p class=institution> Bremen University </p>
                            </div>
                        </a>
                        <a href="https://ishikasingh.github.io/" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/ishika.png class="figure-img img-fluid ">
                                <p class=profname> Ishika Singh </p>
                                <p class=institution> University of Southern California </p>
                            </div>
                        </a>
                        <a href="https://siddhanthaldar.github.io/" target="_blank">
                            <div class="profpic xlarge-1 columns">
                                <img src=./images/organizers/siddhant.png class="figure-img img-fluid ">
                                <p class=profname> Siddhant Haldar </p>
                                <p class=institution> New York University </p>
                            </div>
                        </a>
                    </div>
                </div>
            </div>
    </section>
    <br>
    <hr class="half-rule" />
    <!-- section id="tpc">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<span class=titlesec>Program Committee</span><br>


<div class="pc-column">
<ul>
<li>Ursula Addison</li>
<li>Harel Biggie</li>
<li>Gertjan Burghouts</li>
<li>Nicolas Chapman</li>
<li>Bingqing Chen (<b>MR</b>)</li>
<li>Jonathan Francis (<b>Ch</b>)</li>
<li>Ruihan Gao</li>
<li>Nikolaos Gkanatsios</li>
<li>Weiwei Gu</li>
<li>Binghao Huang</li>
<li>Nathan Hughes</li>
<li>Hanxiao Jiang</li>
<li>Nikhil Keetha</li>
<li>Anne Kemmeren</li>

</ul>
</div>

<div class="pc-column" style="margin:0 30px 0 0;">
<ul>
<li>Seungchan Kim</li>
<li>N. Suresh K. Kondepudi</li>
<li>Seungjae Lee</li>
<li>Tabitha Lee (<b>TR</b>)</li>
<li>Qiang Li (<b>ER</b>)</li>
<li>Zhixuan Liu</li>
<li>Aaron Lohner</li>
<li>Xiaopeng Lu</li>
<li>Dominic Maggio</li>
<li>Nicolas Marticorena Vidal</li>
<li>Andrew Melnik</li>
<li>Marius Memmel</li>
<li>Mark Mints</li>
<li>Daniel Omeiza (<b>TR</b>)</li>
</ul>
</div>

<div class="pc-column">
<ul>
<li>Alessandro Oltramari</li>
<li>Karthik Paga</li>
<li>Maithili Patel</li>
<li>Sarvesh Patil</li>
<li>Andrey Rudenko</li>
<li>Ateeq Sharfuddin</li>
<li>Roykrong Sukkerd (<b>TR</b>)</li>
<li>Shivam Vats</li>
<li>Rui Wang (<b>TR</b>)</li>
<li>Ho-Hsiang Wu (<b>MR</b>)</li>
<li>Yaqi Xie</li>
<li>Junjie Ye</li>
<li>Jiarui Zhang</li>
<li>Yufei Zhu</li>
</ul>
</div>
<br>
<br>
<b>ER</b> â€” <i>Recognises PC member who served as an Emergency Reviewer.</i><br>
<b>TR</b> â€” <i>Recognises PC member who, according to Workshop Chairs' ratings, ranked in the top 10% of all Reviewers.</i><br>
<b>MR</b> â€” <i>Recognises PC member who provided their services as a Meta-Reviewer.</i><br>
<b>Ch</b> â€” <i>Paper Track Chair.</i>
			    
        	    </div>
		</div>
		</div>
	</section> -->
    <!-- hr class="half-rule"/> -->
    <section id="acknowledgement">
        <div class="container">
            <div class="row">
		    <span class="titlesec">Acknowledgement</span>
    <div style="display: flex; justify-content: space-between; align-items: center;">
	    <img src="https://vib.ai.uni-bremen.de/img/fame_logo_rgb.jpg" style="width: 32%; height: auto;">
	    <img src="https://www.eurobin-project.eu/images/euROBIN_img/Ai%20noe_euROBIN_logo.png" style="width: 32%; height: auto;">
	    <img src="https://soc.kuleuven.be/lines/euexact/images/eu-logo.jpg/@@images/image.jpeg" style="width: 32%; height: auto;">
    </div>	    
		    <span style="justify-content: left; text-align:left;"> This workshop is supported by the Research Initiative FAME (Future-oriented cognitive Action Modelling Engine) and the European Network of Excellence Centers in Robotics <a href="https://www.eurobin-project.eu">euROBIN</a>.</span>
            </div>
        </div>
    </section>
<hr class="half-rule" />
<section id="contact">
        <div class="container">
            <div class="row">
		    <span class="titlesec">Contact and Information</span>
    <div style="display: flex; justify-content: space-between; align-items: center;">
	    <img src="https://semrob.github.io/images/semrob_2025_logo.png" style="width: 32%; height: auto;">
    </div>	    
		    <span style="justify-content: left; text-align:left;"><br> Direct questions to <a href="mailto:semrob.workshop+general@gmail.com">semrob.workshop+general@gmail.com</a>. Subscribe to our <a target="_blank" href="https://mailchi.mp/07f8ab4c1c65/semrob-workshop">mailing list</a> to stay updated.</span>
            </div>
        </div>
</section>	
	
	<!-- <hr class="half-rule"/>
	<section id="sponsors">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
		<span class=titlesec>Sponsors</span><br>
				<img  src=../images/sponsors/nvidia.png style="width:400px;height:220px;">
	    </div>
		</div>
		</div>
	</section> -->
    <!-- Footer -->
    <!-- Bootstrap core JavaScript -->
    <!-- 	<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
	<script src="vendor/jquery/jquery.min.js"></script>
	<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
	<!-- Plugin JavaScript -->
    <!-- <script src="vendor/jquery-easing/jquery.easing.min.js"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"> </script>
    <!-- Custom JavaScript for this theme -->
    <script src="js/scrolling-nav.js"></script>
</body>

</html>
