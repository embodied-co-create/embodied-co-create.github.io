<!DOCTYPE html>
<html lang="en">
    <head>
	<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap" rel="stylesheet">
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<meta name="description" content="">
	<meta name="author" content="">
	<title>RSS SemRob 2024</title>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

	  <meta charset="utf-8">
	  <meta name="viewport" content="width=device-width, initial-scale=1">
	  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
	  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
	  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
	  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

	<!-- Latest compiled and minified JavaScript -->
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


	<link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
	<!-- Custom styles for this template -->


	<link href="./css/scrolling-nav.css" rel="stylesheet">
	<link href="./css/style.css" rel="author stylesheet">

	<style>

		.pc-column {
		    width:270px;
		    display:inline-block;
		    vertical-align: top;
		}
		
		.pc_list_item {
		    display:inline-block;
		    width:200px;
		}
	
	</style>
	    
    </head>

    <body id="page-top">

	<!-- Navigation -->
	<nav class="navbar navbar-expand-lg navbar-light bg-light" id="mainNav">
	    <div class="container bar-container">
		<a class="title-head" href="#page-top">RSS SemRob 2024</a>
		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
		    <span class="navbar-toggler-icon"></span>
		</button>
		<div class="collapse navbar-collapse" id="navbarResponsive">
		    <ul class="navbar-nav ml-auto">
				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#about">About</a>
				</li>

				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#speakers">Speakers</a>
				</li>

				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#schedule">Schedule</a>
				</li>

 				<!-- <li class="nav-item">
                    		    <a class="nav-link js-scroll-trigger" href="#callpapers">Call for Papers </a>
				</li> --> 

				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#papers">Accepted Papers</a>
		    		</li>

				<!-- <li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#faq">FAQ</a>
				</li> -->
				<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="#organizers">Organizers</a>
				</li>
			    	<li class="nav-item">
				    <a class="nav-link js-scroll-trigger" href="/">(Most Recent Instance)</a>
				</li>
		    </ul>
		</div>
	    </div>
	</nav>

	<header class="headercontainer bg-primary text-white" style="padding: 0%; max-height: none; ">
	    <div style="background-color: rgba(160,160,160,0.0)" class="text-center">
		<div style="padding-bottom: 6%; padding-top: 6%; background-image: url('./images/banner_full.png'); background-size: cover; background-position: center">

	    <div class="container titlebox"; style="display: inline-block; background-color:rgba(0,0,0, 0.7); width:auto;">
		<p style="text-align: center; margin-bottom: 2" class="title">1st Workshop on <b></b>Semantic Reasoning and Goal Understanding in Robotics</b> (SemRob)</p>
		<p style="text-align: center; margin-bottom: 0" class="subtitle">Robotics Science and Systems Conference (RSS 2024) - July 19 - Delft, Netherlands</p>
		<!-- p style="text-align: center; margin-bottom: 0" class="subtitle"><a href="https://cmu.zoom.us/j/96721732398?pwd=IjXlfZA9Nveip8RP0IrUGP080NaCsA.1" target="_blank">[<b>Zoom</b>]</a> <a href="https://app.sli.do/event/2YZ5kgotSaPWcEEgSiYS6H" target="_blank">[<b>Submit Speaker/Debate Questions</b>]</a></p -->

		</div>
	    </div>
	    </div>
	</header>

	    <hr class="half-rule"/>
	<section id="about" style="padding:70px 0 0 0">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<!-- div style="justify-content: center; text-align:center;"><img src="https://semrob.github.io/images/semrob_2024_logo.png" width="700px"></div-->
			<span class=titlesec>About</span>
			<br>
			<span>
				Semantic understanding of the world is essential for robots to make safe and informed decisions, to adapt to changing environmental conditions, and to enable efficient interactions with other agents. In pursuit of semantic understanding, agents must be able to (i) interpret and represent high-level goals, agnostic of their physical morphology and despite irrelevant aspects of their environments; they must be able to (ii) reason, i.e., to extract abstract concepts from observations in the real-world, logically manipulate these concepts, then leverage the results for inference on downstream tasks; and they must be able to (iii) execute morphology-, environment-, and socially-appropriate behaviors towards those high-level goals. </b>
				</br>
			</br>

				Despite substantial recent advancements in the use of pre-trained, large-capacity models (i.e., foundation models) for difficult robotics problems, methods still struggle in the face of several practical challenges that relate to real-world deployment, e.g., cross-domain generalization, adaptation to dynamic and human-shared environments, and lifelong operation in open-world contexts. This workshop intends to sponsor discussion of new hybrid methodologies—those that combine representations from foundation models with modeling mechanisms that may prove useful for semantic reasoning and abstract goal understanding, including neural memory mechanisms, procedural modules (e.g., cognitive architectures), neuro-symbolic representations (e.g., knowledge/scene graph embeddings), chain-of-thought reasoning mechanisms, robot skill primitives and their composition, 3D scene representations (e.g., NeRFs), etc.
			</br>
		</br>
			<i>Intended audience.</i> We aim to bring together engineers, researchers, and practitioners from different communities to enable avenues for interdisciplinary research on methods that could facilitate the deployment of semantics-aware and generalizable embodied agents in unstructured and dynamic real world environments. In addition to the organizers, the presenters, panelists, and technical program committee are drawn from the following (sub-)communities: Robot Learning, Embodied AI, Planning + Controls, Cognitive Robotics, Neuro-Symbolism, Natural Language Processing, Computer Vision, and Multimodal Machine Learning. We likewise intend to attract an audience from these diverse sub-communities to contribute to compelling discussions.
				</span>

		    </div>
		</div>

	    </div>



	</section>

	<section id="event">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<span class=titlesec>Event Information</span>
			<span>
			This is a primarily in-person workshop, held at the <i>2024 Robotics Science and Systems conference</i> (RSS), in Delft, Netherlands on <b>19 July 2024</b>, starting at <b>13:30 CET</b>.
			</br>
			</br>
			The room location is <a href="https://esviewer.tudelft.nl/space/28/" target="_blank"><b>Lecture Hall D</b></a>, in the <a href="https://map.tudelftcampus.nl/poi/aula-conference-centre/" target="_blank"><b>Aula Conference Centre</b></a>, at TU Delft. You might find the <a href="https://roboticsconference.org/attending/travel/" target="_blank">travel information</a> on the RSS 2024 website helpful.
			</br>
			</br>
			Poster session location: outside the lower right-side doors of <b>Lecture Hall D</b>.
			</br>
			</br>
			RSS SemRob 2024 is <b>Workshop #28</b>, on the official RSS schedule.
			</span>

		    </div>
		</div>

	    </div>



	</section>
	


	<hr class="half-rule"/>
	
	<section id="speakers">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
		<span class=titlesec> Speakers and Panelists</span><br>
		<div class="row">

			<a href="https://web.eecs.umich.edu/~chaijy/" target="_blank">
				<div class="profpic speaker xlarge-1 columns">
					 <img  src="./images/speakers/chai.jpg" class="figure-img img-fluid ">
				 <p class=profname> Joyce Chai </p>
				 <p class=institution>  University of Michigan </p>
	 
				 </div>
			</a>

			<a href="https://nikosuenderhauf.github.io/" target="_blank">
		    <div class="profpic speaker xlarge-1 columns">
			    <img  src="./images/speakers/niko.jpg" class="figure-img img-fluid ">
			<p class=profname> Niko Sünderhauf </p>
			<p class=institution> QUT Centre for Robotics </p>
		    </div>
			</a>
			
			<a href="https://talkingtorobots.com/yonatanbisk.html" target="_blank">
		    <div class="profpic speaker xlarge-1 columns">
			    <img  src='./images/speakers/bisk.jpg' class="figure-img img-fluid ">
			<p class=profname> Yonatan Bisk </p>
			<p class=institution> Carnegie Mellon University </p>

		    </div>
			</a>

			</a>
			<a href="https://www.iit.it/people-details/-/people/alessandra-sciutti" target="_blank">
		    <div class="profpic speaker xlarge-1 columns">
			    <img  src='./images/speakers/sciutti.jpeg' class="figure-img img-fluid ">
			<p class=profname> Alessandra Sciutti </p>
			<p class=institution>Istituto Italiano di Tecnologia</p>

		    </div>
			</a>
		</div>
		<div class="row">
			<a href="https://cdevin.github.io/" target="_blank">
		   <div class="profpic speaker xlarge-1 columns">
			    <img  src="./images/speakers/devin.jpg" class="figure-img img-fluid ">
			<p class=profname> Coline Devin </p>
			<p class=institution>  Google DeepMind  </p>

		    </div>
			</a>

			<a href="https://animesh.garg.tech/" target="_blank">
		   <div class="profpic speaker xlarge-1 columns">
			    <img  src="./images/speakers/animesh.jpg" class="figure-img img-fluid ">
			<p class=profname> Animesh Garg </p>
			<p class=institution>  Georgia Tech </p>

		    </div>
			</a>

			<a href="https://yilundu.github.io/" target="_blank">
		   <div class="profpic speaker xlarge-1 columns">
			    <img  src="./images/speakers/yilun3.png" class="figure-img img-fluid ">
			<p class=profname> Yilun Du </p>
			<p class=institution>  MIT -> Harvard (<i>Incoming AP</i>) </p>

		    </div>
			</a>
		</div>


		</div>
</div>

</section>

	<hr class="half-rule"/>
	<section class="">
	    <div class="container" id="schedule">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<span class="titlesec"><span></span>Schedule</span>
			<br><br>
			<table class="table table-striped">
				<tbody>
				<tr>
					<th style="width: 21%"> Time </th>
				</tr>
				<tr>
					<td style="width: 21%">	13:30        <td/><td>  Organizers <br> <b> Introductory Remarks  </b> </td>
				</tr>

				<tr>
					<td style="width: 21%">	13:40        <td/><td>  Keynote 1: Niko Sünderhauf <br> <b> Semantics and Understanding for Better Perception, Representation, and Actions </b>

 					<br>
 					<a data-toggle="collapse" data-target="#abstractmark" class="collapsed abstract" aria-expanded="false"> Abstract</a>
								<div id="abstractmark" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									 What makes a good world representation for robotics that captures physical and visual properties, can predict the outcomes of robot interactions, and can be initialised and updated in real-time from vision? How can we learn long-horizon tasks involving interactions between multiple objects from only 10 demonstrations, while generalising across spatial and intra-class variations? And is the open-set problem still alive in the age of vision-language models? In this talk, I will touch on three of our recent projects in the areas of perception, representation, and robot learning.
								</div>
					</td>
				</tr>

				<tr>
					<td style="width: 21%">	14:00        <td/><td>    Keynote 2: Alessandra Sciutti <br> <b> Toward Artificial Cognition </b>

 					<br>
 					<a data-toggle="collapse" data-target="#abstractjacob" class="collapsed abstract" aria-expanded="false"> Abstract</a>
								<div id="abstractjacob" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									 Developing autonomous robotic agents with cognitive capabilities might benefit from a brain-inspired embodied cognitive approach, focused on proactive knowledge acquisition and on bidirectional human-robot interaction. Some of the building blocks of such artificial cognition are the principles of developmental robotics, the prospection capabilities, and the crucial role of social interaction. In this talk I will touch some of these aspects, with particular reference to understanding others.
								</div>
					</td>
				</tr>

				<tr>
					<td style="width: 21%">	14:20      <td/><td>  Keynote 3: Coline Devin <br> <b> Free lunch? Revisiting tradeoffs in goal-conditioned policy learning in the foundation model era </b>

 					<br>
 					<a data-toggle="collapse" data-target="#abstractlerrel" class="collapsed abstract" aria-expanded="false"> Abstract</a>
								<div id="abstractlerrel" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
									 Goal-conditioned policy learning can look like very different dependending on choices in goal abstraction (state/image/language), data complexity, and human supervision.This talk will explore whether the advent of powerful foundation models fundamentally changes the tradeoffs associated with these choices, potentially offering more efficient and effective ways to train general robot agents.
								</div>
					</td>
				</tr>

				<tr>
					<td style="width: 21%">	14:40        <td/><td>  Spotlight Talks. IDs: 4,5,26,27 </td>
				</tr>

				<tr>
					<td style="width: 21%">	15:10        <td/><td>    Keynote 4: Animesh Garg <br> <b> A Perspective on Prospection for Robot Autonomy </b>
 					<br>
 					<a data-toggle="collapse" data-target="#abstractmatthew" class="collapsed abstract" aria-expanded="false"> Abstract</a>
					<div id="abstractmatthew" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
						We will discuss a concept of prospection as the ability to predict  two types of generative models: change in the environment (“What”) and morphology-dependent goal conditioned control (“How”). I will provide examples of recent work which move us towards a vision of large scale pretrained models for both stages in the context of robotics, and specifically for contact-rich manipulation.
					</div>

					</td>
				</tr>
				
				<tr>
					<td style="width: 21%">	15:30        <td/><td>  Coffee Break, Socializing, Posters </td>
				</tr>

				<tr>
					<td style="width: 21%">	16:20        <td/><td>  Keynote 5: Joyce Chai <br> <b> LLMs for Navigation and Grounding in Cognitive Robots </b>

						<br>
						<a data-toggle="collapse" data-target="#abstractlerrel" class="collapsed abstract" aria-expanded="false"> Abstract</a>
								   <div id="abstractlerrel" class="news collapse" style="margin-top: 10px; height: 22px;" aria-expanded="false">
										The rise of large foundation models and generative AI have revolutionized many aspects of cognitive robots. In this talk, I will give a brief introduction to our recent work that applies LLMs to coordinate language communication with cognitive robots, particularly in personalized navigation and 3D grounding.
								   </div>
					   </td>
				</tr>
				
				<tr>
					<td style="width: 21%">	16:40        <td/><td>  Debate: Implicit/Data-emergent Reasoning Capabilities versus Explicit Reasoning Mechanisms? <br> <b> Panelists: Animesh Garg, Coline Devin, Yonatan Bisk, Niko Sünderhauf, Yilun Du</b> </td>
				</tr>

				<tr>
					<td style="width: 21%">
						17:50    <td/><td>   Organizers <br>  <b>    Closing Remarks </b>
					</td>
				</tr>

				</tbody>

			</table>
		    </div>
		</div>
	    </div>
	</section>



	<br>
	<hr class="half-rule"/>
<section id="callpapers" style="padding: 70px 0 0 0;">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
		<span class=titlesec>Call for Papers</span><br>
			<h5 style="font-weight: bold"> Targeted Topics </h5>
            In addition to the <a target="_blank" href="https://roboticsconference.org/information/cfp/">RSS 2024 subject areas</a>, we especially invite paper submissions on various topics, including (but not limited to):
			<br><br>
            <ul>
                <li>Learning semantically-rich and generalizable robot state representations </li>
                <li>Learning general goal representations, e.g., in instruction-following</li>
                <li>Reasoning mechanisms for generalization in open-vocabulary contexts</li>
		<li>Leveraging foundation models for robotics tasks; efforts to create robotics-specific foundation models</li>
		<li>Foundation model agent frameworks, e.g., for chain-of-thought reasoning, self-guidance, reasoning about failures, policy-refinement, etc.</li>
		<li>Multimodal tokenization and prompt mechanisms with foundational models for robotics tasks</li>
		<li>Grounding foundation models with other modalities (e.g., haptics, audio, IMU signals, joint torques, etc.)</li>
		<li>Combining foundation models with AI reasoning structures (e.g., neuro-symbolic structures, memory, cognitive architectures, etc.), for robotics tasks</li>
		<li>Data-efficient concept learning for robotics, e.g., few-shot demonstrations, interactive perception, co-simulation, etc.</li>
            </ul>
			<h5 style="font-weight: bold"> Submission Guidelines </h5>
            RSS SemRob 2024 suggests <b>4+N or 8+N paper length</b> formats — i.e., 4 or 8 pages of main content with unlimited additional pages for references, appendices, etc. However, like RSS 2024, we impose no strict page length requirements on submissions; we trust that authors will recognize that respecting reviewers’ time is helpful to the evaluation of their work.<br><br>
            Submissions are handled through CMT: <a href="https://cmt3.research.microsoft.com/SEMROB2024" target="_blank">https://cmt3.research.microsoft.com/SEMROB2024</a><br><br>
            We will accept the official <a target="_blank" href="https://roboticsconference.org/docs/paper-template-latex.tar.gz">LaTeX</a> or <a target="_blank" href="https://roboticsconference.org/docs/paper-template-word.zip">Word</a> paper templates, provided by RSS 2024.
			<br><br>
	    Our review process will be <b>double-blind</b>, following the RSS 2024 paper submission policy for Science/Systems papers.
			    <br><br>
            All accepted papers will be invited for poster presentations; the highest-rated papers, according to the Technical Program Committee, will be given spotlight presentations. Accepted papers will be made available online on this workshop website as <b>non-archival</b> reports, allowing authors to also submit their works to future conferences or journals. We will highlight the Best Reviewer and reveal the Best Paper Award during the closing remarks at the workshop event.<br><br>
			<h5 style="font-weight: bold"> Important Dates </h5>
			<ul>

			<li style="display: list-item">
				<s><b>Submission deadline:</b> <font color="#FF0000"><b>10 June 2024</b></font>, 23:59 AOE.</s>
			</li>
			<li style="display: list-item">
				<s><b>Author Notifications:</b> 24 June 2024, 23:59 AOE.</s>

			</li>
				<li style="display: list-item">
				<s><b>Camera Ready:</b> 1 July 2024, 23:59 AOE.</s>

			</li>
					<li style="display: list-item">
					<b>Workshop:</b> 19 July 2024, 13:30-18:00 CET

			</li>
			</ul>
	    </div>
		</div>
			</div>
		    </div>
		</div>
	</section>


	<hr class="half-rule"/>
	<section id="papers">
		<div class="container">
			<div class="row">
			    <div class="col-md-10 mx-auto">
					<span class=titlesec>Accepted Papers</span><br>

				    	<p>Congratulations to Haoran Geng (SAGE: Bridging Semantic and Actionable Parts for Generalizable Manipulation of Articulated Objects) for winning the Best Paper Award!</p>

					<ul class="listpapers">
						<li>

							(Paper ID #2) la VIDA: Towards a Motivated Goal Reasoning Agent
							 <a class="linkpaper" href="./docs/rss_semrob2024_cr_paper2.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Ursula Addison
							</span>
						</li>
						<li>
							 (Paper ID #3) Open6DOR: Benchmarking Open-instruction 6-DoF Object Rearrangement and A VLM-based Approach
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper3.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Yufei Ding, Haoran Geng, Chaoyi Xu, Xiaomeng Fang, Jiazhao Zhang, Songlin Wei, Qiyu Dai, Zhizheng Zhang, He Wang
							</span>
						</li>
						<li>
								(Paper ID #4) SAGE: Bridging Semantic and Actionable Parts for Generalizable Manipulation of Articulated Objects
								<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper4.pdf" target="_blank"> [paper]</a>
								<a class="linkpaper" href="https://youtu.be/hLJlRt_MZEg?si=n1uTE60nN-y9pgTN" target="_blank"> [video]</a>
								<span style="color: #ff8c00">(spotlight)</span>
								<span style="color: #ff0000">(best paper award!)</span><br>
							<span class="authorname">Haoran Geng, Songlin Wei, Congyue Deng, Bokui Shen, He Wang, Leonidas Guibas</span>
						</li>
						<li>
								(Paper ID #5) Behavior Generation with Latent Actions 
								 <a class="linkpaper" href="./docs/rss_semrob2024_cr_paper5.pdf" target="_blank"> [paper]</a>
								<a class="linkpaper" href="https://youtu.be/UfFBXEeNbgk?si=MkfoBQjNCKIBuWOw" target="_blank"> [video]</a>
							<span style="color: #ff8c00">(spotlight)</span><br>
							<span class="authorname">Seungjae Lee, Yibin Wang, Haritheja Etukuru, Hyoun Jin Kim, Nur Muhammad (Mahi) Shafiullah, Lerrel Pinto</span>
						</li>
						<li>
							(Paper ID #6) Language models are robotic planners: reframing plans as goal refinement graphs
							 <a class="linkpaper" href="./docs/rss_semrob2024_cr_paper6.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Ateeq Sharfuddin, Travis Breaux
							</span>
						</li>
						<li>
							(Paper ID #7) OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper7.pdf" target="_blank"> [paper]</a>
						 <br>
							<span class="authorname">
							Peiqi Liu, yaswanth kumar Orru, Jay Vakil, Christopher Paxton, Nur Muhammad (Mahi) Shafiullah, Lerrel Pinto
							</span>
						</li>
						<li>
							(Paper ID #9) Clio: Real-time Task-Driven Open-Set 3D Scene Graphs
							  <a class="linkpaper" href="./docs/rss_semrob2024_cr_paper9.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Dominic Maggio, Yun Chang, Nathan Hughes, Matthew Trang, John Griffith, Eric Cristofalo, Carlyn Dougherty, Lukas Schmid, Luca Carlone
							</span>
						</li>


						<li>
							(Paper ID #10) Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper10.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta
							</span>
						</li>
						<li>
							(Paper ID #12) VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper12.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Haochen Zhang, Nader Zantout, Pujith Kachana, Zongyuan Wu, Ji Zhang, Wenshan Wang
							</span>
						</li>
						<li>

							(Paper ID #14) Enhancing Vision-Language Models with Scene Graphs for Traffic Accident Understanding
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper14.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Aaron Lohner, Francesco Compagno, Jonathan Francis, Alessandro Oltramari
						</span>
						</li>

						<li>

							(Paper ID #15) Which objects help me to act effectively? Reasoning about physically-grounded affordances
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper15.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Anne Kemmeren, Gertjan Burghouts, Michael van Bekkum, Wouter Meijer, Jelle van Mil
						</span>
						</li>

						<li>

							(Paper ID #17) RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper17.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Yuxuan Kuang, Junjie Ye, Haoran Geng, Jiageng Mao, Congyue Deng, Leonidas Guibas, He Wang, Yue Wang
						</span>
						</li>

						<li>

							(Paper ID #19) Dialog-based Skill and Task Learning for Robot
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper19.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Weiwei Gu, N. Suresh K. Kondepudi, Lixiao Huang, Nakul Gopalan
						</span>
						</li>

						<li>

							(Paper ID #20) Embodied AI Robot Companion for Efficient Object Handling in Bimanual Teleoperation
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper20.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Haolin Fei, Songlin Ma, Bo Xiao, Ziwei Wang
						</span>
						</li>

						<li>

							(Paper ID #21) Grounding Language Plans in Demonstrations Through Counterfactual Perturbations
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper21.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Yanwei Wang
						</span>
						</li>

						<li>

							(Paper ID #22) RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper22.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Hanxiao Jiang, Binghao Huang, Ruihai Wu, Zhuoran Li, Shubham Garg, Hooshang Nayyeri, Shenlong Wang, Yunzhu Li
						</span>
						</li>

						<li>

							(Paper ID #23) Lang2LTL-2: Grounding Spatiotemporal Navigation Commands Using Large Language and Vision-Language Models
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper23.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Jason Xinyu Liu, Ankit Shah, George Konidaris, Stefanie Tellex, David Paulius
						</span>
						</li>

						<li>

							(Paper ID #24) CogExplore: Contextual Exploration with Language Encoded Environment Representations
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper24.pdf" target="_blank"> [paper]</a>
							<br>
							<span class="authorname">
							Harel Biggie, Patrick Cooper, Doncey Albin, Kristen Such, Christoffer Heckman
						</span>
						</li>
						<li>
								(Paper ID #26) Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation
								<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper26.pdf" target="_blank"> [paper]</a>
								<a class="linkpaper" href="https://youtu.be/69CKzlAxUAY?si=XrdpCnQZRNkdU3gq" target="_blank"> [video]</a>
								<span style="color: #ff8c00">(spotlight)</span><br>
								<span class="authorname">Daniel Honerkamp, Martin Buechner, Fabien Despinoy, Tim Welschehold, Abhinav Valada</span>
						</li>


						<li>
							(Paper ID #27) Natural Language Can Help Bridge the Sim2Real Gap
							<a class="linkpaper" href="./docs/rss_semrob2024_cr_paper27.pdf" target="_blank"> [paper]</a>
							<a class="linkpaper" href="https://youtu.be/-CwOOz1Gnks?si=slG8QZYVvMrjl5RI" target="_blank"> [video]</a>
							<span style="color: #ff8c00">(spotlight)</span><br>
							<span class="authorname">Albert Yu, Adeline Foote, Raymond Mooney, Roberto Martín-Martín</span>
						</li>
					</ul>

				</div>
			</div>
		</div>
	</section>





	<hr class="half-rule"/>
	<section id="organizers">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">

		<span class=titlesec>Organizers</span><br>
		<div class="row">
			
            <a href="#">
		    <div class="profpic xlarge-1 columns">
			<img src=./images/organizers/jonathan.png class="figure-img img-fluid ">
			<p class=profname><a href="#">Jonathan Francis</a></p>
			<p class=institution>Bosch Center for Artificial Intelligence</p>
		    </div>
			</a>

            <a href="#">
		    <div class="profpic xlarge-1 columns">
			<img  src=./images/organizers/melnik.png class="figure-img img-fluid ">
			<p class=profname>  <a href="https://www.linkedin.com/in/andrewmelnik">Andrew Melnik</a> </p>
			<p class=institution> University of Bielefeld </p>
		    </div>
			</a>
			
            <a href="#">
		    <div class="profpic xlarge-1 columns">
			    <img src=./images/organizers/krishan.jpg class="figure-img img-fluid ">
			<p class=profname><a href="#"> Krishan Rana </a> </p>
			<p class=institution> QUT Centre for Robotics </p>
		    </div>
			</a>

	    <a href="#">
		    <div class="profpic xlarge-1 columns">
			    <img  src='./images/organizers/saxena.jpg' class="figure-img img-fluid ">
			<p class="profname"> <a href="#/"> Saumya Saxena </a></p>
			<p class=institution> Carnegie Mellon University</p>
		    </div>
			</a>

		</div>
		<div class="row">

            <a href="#">
            <div class="profpic xlarge-1 columns">
                <img  src='./images/organizers/ahn.jpg' class="figure-img img-fluid ">
            <p class="profname"> <a href="#/"> Hyemin Ahn </a></p>
            <p class=institution> Ulsan National Institute of Science and Technology</p>
            </div>
            </a>
            <a href="#">
	    <div class="profpic xlarge-1 columns">
		    <img  src='./images/organizers/qiang.jpg' class="figure-img img-fluid ">
		<p class=profname><a href="#"> Qiang Li</a></p>
		<p class=institution>Shenzhen Technology University</p>

	    </div>
	    </a>
            <a href="#">
            <div class="profpic xlarge-1 columns">
                <img  src='./images/organizers/jean.jpg' class="figure-img img-fluid ">
            <p class="profname"> <a href="#/"> Jean Oh </a></p>
            <p class=institution> Carnegie Mellon University</p>
            </div>
            </a>
		</div>


		</div>
</div>

	</section>

	<br>
	<hr class="half-rule"/>
	<section id="tpc">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
			<span class=titlesec>Program Committee</span><br>

				<!-- column 1 -->
<div class="pc-column">
<ul>
<li>Ursula Addison</li>
<li>Harel Biggie</li>
<li>Gertjan Burghouts</li>
<li>Nicolas Chapman</li>
<li>Bingqing Chen (<b>MR</b>)</li>
<li>Jonathan Francis (<b>Ch</b>)</li>
<li>Ruihan Gao</li>
<li>Nikolaos Gkanatsios</li>
<li>Weiwei Gu</li>
<li>Binghao Huang</li>
<li>Nathan Hughes</li>
<li>Hanxiao Jiang</li>
<li>Nikhil Keetha</li>
<li>Anne Kemmeren</li>

</ul>
</div>

<!-- column 2 -->
<div class="pc-column" style="margin:0 30px 0 0;">
<ul>
<li>Seungchan Kim</li>
<li>N. Suresh K. Kondepudi</li>
<li>Seungjae Lee</li>
<li>Tabitha Lee (<b>TR</b>)</li>
<li>Qiang Li (<b>ER</b>)</li>
<li>Zhixuan Liu</li>
<li>Aaron Lohner</li>
<li>Xiaopeng Lu</li>
<li>Dominic Maggio</li>
<li>Nicolas Marticorena Vidal</li>
<li>Andrew Melnik</li>
<li>Marius Memmel</li>
<li>Mark Mints</li>
<li>Daniel Omeiza (<b>TR</b>)</li>
</ul>
</div>

<!-- column 3 -->
<div class="pc-column">
<ul>
<li>Alessandro Oltramari</li>
<li>Karthik Paga</li>
<li>Maithili Patel</li>
<li>Sarvesh Patil</li>
<li>Andrey Rudenko</li>
<li>Ateeq Sharfuddin</li>
<li>Roykrong Sukkerd (<b>TR</b>)</li>
<li>Shivam Vats</li>
<li>Rui Wang (<b>TR</b>)</li>
<li>Ho-Hsiang Wu (<b>MR</b>)</li>
<li>Yaqi Xie</li>
<li>Junjie Ye</li>
<li>Jiarui Zhang</li>
<li>Yufei Zhu</li>
</ul>
</div>
<br>
<br>
<b>ER</b> — <i>Recognises PC member who served as an Emergency Reviewer.</i><br>
<b>TR</b> — <i>Recognises PC member who, according to Workshop Chairs' ratings, ranked in the top 10% of all Reviewers.</i><br>
<b>MR</b> — <i>Recognises PC member who provided their services as a Meta-Reviewer.</i><br>
<b>Ch</b> — <i>Paper Track Chair.</i>
			    
        	    </div>
		</div>
		</div>
	</section>
	    
	<br>
	<hr class="half-rule"/>
	<section id="contact">
	    <div class="container">
	<div class="row">
    <div display="inline"><img src="https://semrob.github.io/images/semrob_2024_logo.png" width="400px"></div>
    <div style="display:inline; width:520px; align-items: center; vertical-align: bottom; padding:20px 0 0 0;">
		<span class="titlesec">Contact and Information</span><br>

	<span style="justify-content: left; text-align:left;">Direct questions to <a href="mailto:semrob.workshop+general@gmail.com">semrob.workshop+general@gmail.com</a>.</span>
	<br><br>
	<span style="justify-content: left; text-align:left;">Subscribe to our <a target="_blank" href="https://mailchi.mp/07f8ab4c1c65/semrob-workshop">mailing list</a> to stay updated on workshop news.</span>

	    </div>
		</div>
		</div>
	</section>


	<!-- <hr class="half-rule"/>
	<section id="sponsors">
	    <div class="container">
		<div class="row">
		    <div class="col-md-10 mx-auto">
		<span class=titlesec>Sponsors</span><br>
				<img  src=../images/sponsors/nvidia.png style="width:400px;height:220px;">
	    </div>
		</div>
		</div>
	</section> -->

	<!-- Footer -->


	<!-- Bootstrap core JavaScript -->
<!-- 	<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
	<script src="vendor/jquery/jquery.min.js"></script>
	<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
	<!-- Plugin JavaScript -->
	<!-- <script src="vendor/jquery-easing/jquery.easing.min.js"></script> --> -->
	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js



"> </script>

	<!-- Custom JavaScript for this theme -->
	<script src="js/scrolling-nav.js"></script>

    </body>


</html>
